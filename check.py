# check.py
"""
ì‹œìŠ¤í…œ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import time
import torch
import pandas as pd
import subprocess
import psutil
from pathlib import Path
import platform
import json
import hashlib
from typing import Dict, List, Tuple, Optional

class UltraHighPerformanceSystemChecker:
    """ì‹œìŠ¤í…œ ì²´í¬"""
    
    def __init__(self):
        self.check_results = {
            "environment": {},
            "files": {},
            "model": {},
            "performance": {},
            "compliance": {},
            "optimization": {},
            "memory": {},
            "gpu_analysis": {}
        }
        self.start_time = time.time()
        
    def run_comprehensive_checks(self):
        """í¬ê´„ì  ì‹œìŠ¤í…œ ê²€ì‚¬"""
        print("=== ê³ ì„±ëŠ¥ ì‹œìŠ¤í…œ ê²€ì¦ ì‹œì‘ ===\n")
        
        # 1. í™˜ê²½ ì²´í¬
        self.check_environment_advanced()
        
        # 2. íŒŒì¼ ì²´í¬
        self.check_files_comprehensive()
        
        # 3. ëª¨ë¸ ì²´í¬
        self.check_model_capabilities()
        
        # 4. GPU ì‹¬ì¸µ ë¶„ì„
        self.analyze_gpu_performance()
        
        # 5. ë©”ëª¨ë¦¬ ìµœì í™” ë¶„ì„
        self.analyze_memory_optimization()
        
        # 6. ì„±ëŠ¥ ì˜ˆì¸¡
        self.estimate_performance_advanced()
        
        # 7. ìµœì í™” ê¸°ëŠ¥ ê²€ì¦
        self.verify_optimization_features()
        
        # 8. ëŒ€íšŒ ê·œì • ì¤€ìˆ˜ ì²´í¬
        self.check_compliance_advanced()
        
        # ìµœì¢… ë³´ê³ ì„œ
        self.generate_comprehensive_report()
    
    def check_environment_advanced(self):
        """ê³ ê¸‰ í™˜ê²½ ê²€ì‚¬"""
        print("1. ê³ ê¸‰ í™˜ê²½ ê²€ì‚¬ ì¤‘...")
        
        # ê¸°ë³¸ ì •ë³´
        self.check_results["environment"]["platform"] = platform.system()
        self.check_results["environment"]["python_version"] = sys.version.split()[0]
        self.check_results["environment"]["pytorch_version"] = torch.__version__
        
        # GPU ìƒì„¸ ë¶„ì„
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            self.check_results["environment"]["gpu_count"] = gpu_count
            
            for i in range(gpu_count):
                gpu_props = torch.cuda.get_device_properties(i)
                gpu_info = {
                    "name": gpu_props.name,
                    "memory_gb": gpu_props.total_memory / (1024**3),
                    "compute_capability": f"{gpu_props.major}.{gpu_props.minor}",
                    "multiprocessor_count": gpu_props.multi_processor_count,
                    "memory_bandwidth": getattr(gpu_props, 'memory_bus_width', 'Unknown')
                }
                self.check_results["environment"][f"gpu_{i}"] = gpu_info
                
                # GPU ì„±ëŠ¥ ë“±ê¸‰ íŒì •
                if gpu_props.total_memory / (1024**3) >= 20:
                    performance_tier = "Ultra High"
                    optimization_mode = "ìµœê³  ì„±ëŠ¥ ëª¨ë“œ"
                elif gpu_props.total_memory / (1024**3) >= 12:
                    performance_tier = "High"
                    optimization_mode = "ê³ ì„±ëŠ¥ ëª¨ë“œ"
                elif gpu_props.total_memory / (1024**3) >= 8:
                    performance_tier = "Medium"
                    optimization_mode = "ê· í˜• ëª¨ë“œ"
                else:
                    performance_tier = "Basic"
                    optimization_mode = "íš¨ìœ¨ì„± ëª¨ë“œ"
                
                gpu_info["performance_tier"] = performance_tier
                gpu_info["recommended_mode"] = optimization_mode
            
            # CUDA ê¸°ëŠ¥ í™•ì¸
            self.check_results["environment"]["cuda_version"] = torch.version.cuda
            self.check_results["environment"]["cudnn_version"] = torch.backends.cudnn.version()
            self.check_results["environment"]["tensor_cores"] = self._check_tensor_cores()
            self.check_results["environment"]["mixed_precision"] = torch.cuda.amp.autocast().__class__.__name__ == 'autocast'
            
        else:
            self.check_results["environment"]["gpu_status"] = "âŒ CUDA ì‚¬ìš© ë¶ˆê°€"
        
        # CPU ì •ë³´
        cpu_info = {
            "cores": psutil.cpu_count(logical=False),
            "threads": psutil.cpu_count(logical=True),
            "frequency_mhz": psutil.cpu_freq().current if psutil.cpu_freq() else "Unknown"
        }
        self.check_results["environment"]["cpu"] = cpu_info
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        memory = psutil.virtual_memory()
        self.check_results["environment"]["ram_gb"] = memory.total / (1024**3)
        self.check_results["environment"]["available_ram_gb"] = memory.available / (1024**3)
        
        print("âœ… ê³ ê¸‰ í™˜ê²½ ê²€ì‚¬ ì™„ë£Œ\n")
    
    def _check_tensor_cores(self) -> bool:
        """Tensor Core ì§€ì› í™•ì¸"""
        try:
            # Tensor CoreëŠ” ì£¼ë¡œ V100, A100, RTX ì‹œë¦¬ì¦ˆì—ì„œ ì§€ì›
            gpu_name = torch.cuda.get_device_name(0).upper()
            tensor_core_gpus = ['V100', 'A100', 'RTX', 'TITAN', 'QUADRO']
            return any(gpu in gpu_name for gpu in tensor_core_gpus)
        except:
            return False
    
    def check_files_comprehensive(self):
        """í¬ê´„ì  íŒŒì¼ ê²€ì‚¬"""
        print("2. í¬ê´„ì  íŒŒì¼ ê²€ì‚¬ ì¤‘...")
        
        required_files = {
            "core_files": {
                "inference.py": "ë©”ì¸ ì¶”ë¡  ì‹¤í–‰ íŒŒì¼",
                "model_handler.py": "ê³ ì„±ëŠ¥ ëª¨ë¸ í•¸ë“¤ëŸ¬",
                "data_processor.py": "ì§€ëŠ¥í˜• ë°ì´í„° ì²˜ë¦¬",
                "prompt_engineering.py": "ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§",
                "knowledge_base.py": "ì „ë¬¸ ì§€ì‹ ë² ì´ìŠ¤",
                "advanced_optimizer.py": "ì´ˆê³ ì„±ëŠ¥ ìµœì í™”",
                "pattern_learner.py": "íŒ¨í„´ í•™ìŠµ ì‹œìŠ¤í…œ"
            },
            "data_files": {
                "test.csv": "í…ŒìŠ¤íŠ¸ ë°ì´í„°",
                "sample_submission.csv": "ì œì¶œ í…œí”Œë¦¿"
            },
            "config_files": {
                "requirements.txt": "ì˜ì¡´ì„± íŒ¨í‚¤ì§€",
                "main.py": "ê°œë°œìš© ë©”ì¸ íŒŒì¼"
            }
        }
        
        all_present = True
        total_size = 0
        
        for category, files in required_files.items():
            self.check_results["files"][category] = {}
            
            for filename, description in files.items():
                if os.path.exists(filename):
                    size_mb = os.path.getsize(filename) / (1024*1024)
                    total_size += size_mb
                    
                    # íŒŒì¼ í’ˆì§ˆ ê²€ì‚¬
                    quality_info = self._analyze_file_quality(filename)
                    
                    self.check_results["files"][category][filename] = {
                        "status": "âœ… ì¡´ì¬",
                        "size_mb": round(size_mb, 2),
                        "quality": quality_info
                    }
                else:
                    self.check_results["files"][category][filename] = {
                        "status": "âŒ ì—†ìŒ",
                        "size_mb": 0,
                        "quality": {}
                    }
                    all_present = False
                    print(f"  âš ï¸ {filename} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        
        self.check_results["files"]["all_present"] = all_present
        self.check_results["files"]["total_size_mb"] = round(total_size, 2)
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        if os.path.exists("test.csv"):
            test_df = pd.read_csv("test.csv")
            data_quality = self._analyze_data_quality(test_df)
            self.check_results["files"]["data_quality"] = data_quality
        
        print("âœ… í¬ê´„ì  íŒŒì¼ ê²€ì‚¬ ì™„ë£Œ\n")
    
    def _analyze_file_quality(self, filename: str) -> Dict:
        """íŒŒì¼ í’ˆì§ˆ ë¶„ì„"""
        quality = {"complexity": "Unknown", "features": []}
        
        if filename.endswith('.py'):
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    # ì½”ë“œ ë³µì¡ë„ ì¶”ì •
                    lines = len(content.splitlines())
                    functions = len(re.findall(r'def\s+\w+', content))
                    classes = len(re.findall(r'class\s+\w+', content))
                    
                    if lines > 1000:
                        quality["complexity"] = "High"
                    elif lines > 500:
                        quality["complexity"] = "Medium"
                    else:
                        quality["complexity"] = "Low"
                    
                    quality["lines"] = lines
                    quality["functions"] = functions
                    quality["classes"] = classes
                    
                    # ê³ ê¸‰ ê¸°ëŠ¥ ê²€ì‚¬
                    advanced_features = {
                        "async_support": "async def" in content,
                        "type_hints": "from typing import" in content,
                        "dataclasses": "@dataclass" in content,
                        "caching": "cache" in content.lower(),
                        "gpu_optimization": "cuda" in content.lower(),
                        "parallel_processing": any(term in content for term in ["ThreadPoolExecutor", "multiprocessing", "concurrent"]),
                        "error_handling": "try:" in content and "except" in content
                    }
                    
                    quality["features"] = [feature for feature, present in advanced_features.items() if present]
                    
            except Exception as e:
                quality["error"] = str(e)
        
        return quality
    
    def _analyze_data_quality(self, df: pd.DataFrame) -> Dict:
        """ë°ì´í„° í’ˆì§ˆ ë¶„ì„"""
        return {
            "row_count": len(df),
            "column_count": len(df.columns),
            "missing_values": df.isnull().sum().sum(),
            "data_types": df.dtypes.to_dict(),
            "memory_usage_mb": df.memory_usage(deep=True).sum() / (1024*1024),
            "sample_questions": df['Question'].head(3).tolist() if 'Question' in df.columns else []
        }
    
    def check_model_capabilities(self):
        """ëª¨ë¸ ê¸°ëŠ¥ ê²€ì‚¬"""
        print("3. ëª¨ë¸ ê¸°ëŠ¥ ê²€ì‚¬ ì¤‘...")
        
        try:
            from transformers import AutoTokenizer
            
            model_name = "upstage/SOLAR-10.7B-Instruct-v1.0"
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ í…ŒìŠ¤íŠ¸
            start_time = time.time()
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            tokenizer_load_time = time.time() - start_time
            
            self.check_results["model"]["name"] = model_name
            self.check_results["model"]["tokenizer_load_time"] = round(tokenizer_load_time, 2)
            self.check_results["model"]["vocab_size"] = tokenizer.vocab_size
            self.check_results["model"]["model_max_length"] = tokenizer.model_max_length
            
            # í† í¬ë‚˜ì´ì € ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
            test_text = "ê°œì¸ì •ë³´ë³´í˜¸ë²•ì— ë”°ë¥¸ ì•ˆì „ì„± í™•ë³´ì¡°ì¹˜ì— ëŒ€í•´ ì„¤ëª…í•˜ì„¸ìš”."
            tokens = tokenizer.tokenize(test_text)
            
            self.check_results["model"]["tokenization_test"] = {
                "input_text": test_text,
                "token_count": len(tokens),
                "tokens_sample": tokens[:10]
            }
            
            # ëª¨ë¸ í¬ê¸° ì¶”ì •
            self.check_results["model"]["estimated_size_gb"] = "~22GB (16bit)"
            self.check_results["model"]["recommended_memory_gb"] = 24
            
            del tokenizer
            
        except Exception as e:
            self.check_results["model"]["error"] = str(e)
            print(f"  âš ï¸ ëª¨ë¸ ì²´í¬ ì˜¤ë¥˜: {e}")
        
        print("âœ… ëª¨ë¸ ê¸°ëŠ¥ ê²€ì‚¬ ì™„ë£Œ\n")
    
    def analyze_gpu_performance(self):
        """GPU ì„±ëŠ¥ ì‹¬ì¸µ ë¶„ì„"""
        print("4. GPU ì„±ëŠ¥ ì‹¬ì¸µ ë¶„ì„ ì¤‘...")
        
        if not torch.cuda.is_available():
            self.check_results["gpu_analysis"]["status"] = "âŒ CUDA ë¶ˆê°€"
            return
        
        gpu_analysis = {}
        
        # ë©”ëª¨ë¦¬ ë²¤ì¹˜ë§ˆí¬
        try:
            # ë©”ëª¨ë¦¬ í• ë‹¹ í…ŒìŠ¤íŠ¸
            memory_test_sizes = [1, 2, 4, 8]  # GB
            memory_results = {}
            
            for size_gb in memory_test_sizes:
                try:
                    # ì„ì‹œ í…ì„œ ìƒì„±
                    elements = int(size_gb * 1024**3 / 4)  # float32 ê¸°ì¤€
                    test_tensor = torch.randn(elements, device='cuda', dtype=torch.float32)
                    
                    memory_results[f"{size_gb}GB"] = "âœ… ì„±ê³µ"
                    del test_tensor
                    torch.cuda.empty_cache()
                    
                except RuntimeError as e:
                    memory_results[f"{size_gb}GB"] = f"âŒ ì‹¤íŒ¨: {str(e)[:50]}"
                    torch.cuda.empty_cache()
                    break
            
            gpu_analysis["memory_allocation_test"] = memory_results
            
            # ì—°ì‚° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            performance_tests = self._run_gpu_performance_tests()
            gpu_analysis["performance_tests"] = performance_tests
            
        except Exception as e:
            gpu_analysis["memory_test_error"] = str(e)
        
        # GPU ìƒíƒœ ì •ë³´
        gpu_analysis["current_memory_gb"] = torch.cuda.memory_allocated() / (1024**3)
        gpu_analysis["max_memory_gb"] = torch.cuda.max_memory_allocated() / (1024**3)
        gpu_analysis["memory_efficiency"] = torch.cuda.memory_efficiency() if hasattr(torch.cuda, 'memory_efficiency') else "Unknown"
        
        self.check_results["gpu_analysis"] = gpu_analysis
        
        print("âœ… GPU ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ\n")
    
    def _run_gpu_performance_tests(self) -> Dict:
        """GPU ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        tests = {}
        
        try:
            # í–‰ë ¬ ê³±ì…ˆ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            size = 2048
            a = torch.randn(size, size, device='cuda', dtype=torch.float16)
            b = torch.randn(size, size, device='cuda', dtype=torch.float16)
            
            # Warm-up
            for _ in range(5):
                _ = torch.matmul(a, b)
            torch.cuda.synchronize()
            
            # ì‹¤ì œ ì¸¡ì •
            start_time = time.time()
            for _ in range(10):
                result = torch.matmul(a, b)
            torch.cuda.synchronize()
            end_time = time.time()
            
            avg_time = (end_time - start_time) / 10
            gflops = (2 * size**3) / (avg_time * 1e9)
            
            tests["matrix_multiplication"] = {
                "avg_time_ms": round(avg_time * 1000, 2),
                "gflops": round(gflops, 2),
                "performance_tier": "High" if gflops > 100 else "Medium" if gflops > 50 else "Low"
            }
            
            # Mixed Precision í…ŒìŠ¤íŠ¸
            if torch.cuda.amp.autocast().__class__.__name__ == 'autocast':
                with torch.cuda.amp.autocast():
                    start_time = time.time()
                    for _ in range(10):
                        result = torch.matmul(a.float(), b.float())
                    torch.cuda.synchronize()
                    end_time = time.time()
                
                amp_time = (end_time - start_time) / 10
                tests["mixed_precision"] = {
                    "avg_time_ms": round(amp_time * 1000, 2),
                    "speedup_vs_fp32": round(avg_time / amp_time, 2) if amp_time > 0 else "N/A"
                }
            
            # ì •ë¦¬
            del a, b, result
            torch.cuda.empty_cache()
            
        except Exception as e:
            tests["error"] = str(e)
        
        return tests
    
    def analyze_memory_optimization(self):
        """ë©”ëª¨ë¦¬ ìµœì í™” ë¶„ì„"""
        print("5. ë©”ëª¨ë¦¬ ìµœì í™” ë¶„ì„ ì¤‘...")
        
        memory_analysis = {}
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
        memory = psutil.virtual_memory()
        memory_analysis["system_memory"] = {
            "total_gb": round(memory.total / (1024**3), 1),
            "available_gb": round(memory.available / (1024**3), 1),
            "usage_percent": memory.percent
        }
        
        # GPU ë©”ëª¨ë¦¬ (CUDA ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            memory_analysis["gpu_memory"] = {
                "total_gb": round(gpu_memory, 1),
                "recommended_usage_gb": round(gpu_memory * 0.9, 1),
                "optimization_strategies": self._get_memory_optimization_strategies(gpu_memory)
            }
        
        # ìµœì í™” ê¶Œì¥ì‚¬í•­
        memory_analysis["optimization_recommendations"] = self._generate_memory_recommendations()
        
        self.check_results["memory"] = memory_analysis
        
        print("âœ… ë©”ëª¨ë¦¬ ìµœì í™” ë¶„ì„ ì™„ë£Œ\n")
    
    def _get_memory_optimization_strategies(self, gpu_memory_gb: float) -> List[str]:
        """ë©”ëª¨ë¦¬ ìµœì í™” ì „ëµ"""
        strategies = []
        
        if gpu_memory_gb >= 20:
            strategies.extend([
                "ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ê°€ëŠ¥",
                "ëª¨ë¸ ì»´íŒŒì¼ ìµœì í™” í™œì„±í™”",
                "ê³ í•´ìƒë„ Mixed Precision ì‚¬ìš©",
                "ìºì‹œ í¬ê¸° í™•ëŒ€"
            ])
        elif gpu_memory_gb >= 12:
            strategies.extend([
                "ì¤‘ê°„ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©",
                "ì„ íƒì  ëª¨ë¸ ì»´íŒŒì¼",
                "í‘œì¤€ Mixed Precision",
                "ì ì‘í˜• ìºì‹œ ê´€ë¦¬"
            ])
        else:
            strategies.extend([
                "ì‘ì€ ë°°ì¹˜ í¬ê¸° í•„ìˆ˜",
                "ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ",
                "Gradient Checkpointing ê³ ë ¤",
                "ë¹ˆë²ˆí•œ ìºì‹œ ì •ë¦¬"
            ])
        
        return strategies
    
    def _generate_memory_recommendations(self) -> List[str]:
        """ë©”ëª¨ë¦¬ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            
            if gpu_memory < 12:
                recommendations.append("âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± - ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ í•„ìš”")
                recommendations.append("ğŸ’¡ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ quantization ê³ ë ¤")
            else:
                recommendations.append("âœ… ì¶©ë¶„í•œ GPU ë©”ëª¨ë¦¬")
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì²´í¬
        system_memory = psutil.virtual_memory()
        if system_memory.available / (1024**3) < 8:
            recommendations.append("âš ï¸ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ë¶€ì¡± - ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ê¶Œì¥")
        
        return recommendations
    
    def estimate_performance_advanced(self):
        """ê³ ê¸‰ ì„±ëŠ¥ ì˜ˆì¸¡"""
        print("6. ê³ ê¸‰ ì„±ëŠ¥ ì˜ˆì¸¡ ì¤‘...")
        
        total_questions = 515
        
        # GPU ì„±ëŠ¥ ê¸°ë°˜ ì‹œê°„ ì˜ˆì¸¡
        gpu_info = self.check_results.get("environment", {})
        gpu_memory = 0
        
        if "gpu_0" in gpu_info:
            gpu_memory = gpu_info["gpu_0"]["memory_gb"]
            performance_tier = gpu_info["gpu_0"]["performance_tier"]
        
        # ì„±ëŠ¥ ê³„ì¸µë³„ ì²˜ë¦¬ ì‹œê°„ ì¶”ì •
        if performance_tier == "Ultra High":
            base_time_per_question = 3  # ì´ˆê³ ì„±ëŠ¥
            batch_efficiency = 0.8
        elif performance_tier == "High":
            base_time_per_question = 5  # ê³ ì„±ëŠ¥
            batch_efficiency = 0.7
        elif performance_tier == "Medium":
            base_time_per_question = 8  # ì¤‘ì„±ëŠ¥
            batch_efficiency = 0.6
        else:
            base_time_per_question = 12  # ê¸°ë³¸
            batch_efficiency = 0.5
        
        # ë¬¸ì œ ìœ í˜•ë³„ ë¶„í¬ ì¶”ì •
        estimated_distribution = {
            "ê°ê´€ì‹_ì‰¬ìš´ë¬¸ì œ": int(total_questions * 0.3),    # 30%
            "ê°ê´€ì‹_ë³´í†µë¬¸ì œ": int(total_questions * 0.35),   # 35%
            "ê°ê´€ì‹_ì–´ë ¤ìš´ë¬¸ì œ": int(total_questions * 0.15), # 15%
            "ì£¼ê´€ì‹_ë¬¸ì œ": int(total_questions * 0.2)        # 20%
        }
        
        # ìƒì„¸ ì‹œê°„ ê³„ì‚°
        time_estimates = {}
        total_time = 0
        
        for problem_type, count in estimated_distribution.items():
            if "ì‰¬ìš´" in problem_type:
                time_per_item = base_time_per_question * 0.5
            elif "ë³´í†µ" in problem_type:
                time_per_item = base_time_per_question * 0.8
            elif "ì–´ë ¤ìš´" in problem_type:
                time_per_item = base_time_per_question * 1.2
            else:  # ì£¼ê´€ì‹
                time_per_item = base_time_per_question * 1.5
            
            # ë°°ì¹˜ íš¨ìœ¨ì„± ì ìš©
            if "ê°ê´€ì‹" in problem_type:
                effective_time = time_per_item * batch_efficiency
            else:
                effective_time = time_per_item
            
            category_total = count * effective_time
            time_estimates[problem_type] = {
                "count": count,
                "time_per_question": round(effective_time, 1),
                "total_time": round(category_total, 1)
            }
            total_time += category_total
        
        # ì˜¤ë²„í—¤ë“œ ì¶”ê°€ (ì‹œìŠ¤í…œ ì¤€ë¹„, ë©”ëª¨ë¦¬ ì •ë¦¬ ë“±)
        overhead_time = total_time * 0.15  # 15% ì˜¤ë²„í—¤ë“œ
        total_time_with_overhead = total_time + overhead_time
        
        performance_results = {
            "detailed_estimates": time_estimates,
            "total_processing_time_min": round(total_time_with_overhead / 60, 1),
            "overhead_time_min": round(overhead_time / 60, 1),
            "time_limit_min": 270,  # 4ì‹œê°„ 30ë¶„
            "estimated_questions_per_minute": round(total_questions / (total_time_with_overhead / 60), 1),
            "safety_margin_min": round(270 - (total_time_with_overhead / 60), 1),
            "performance_tier": performance_tier,
            "time_safety": total_time_with_overhead < 270 * 60 * 0.8  # 80% ë§ˆì§„
        }
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡
        model_memory = 22  # GB (ëª¨ë¸ ê¸°ë³¸)
        processing_memory = gpu_memory * 0.2 if gpu_memory > 0 else 4  # ì²˜ë¦¬ìš©
        total_memory_needed = model_memory + processing_memory
        
        performance_results["memory_requirements"] = {
            "model_memory_gb": model_memory,
            "processing_memory_gb": round(processing_memory, 1),
            "total_memory_gb": round(total_memory_needed, 1),
            "available_memory_gb": gpu_memory,
            "memory_safety": gpu_memory >= total_memory_needed if gpu_memory > 0 else False
        }
        
        self.check_results["performance"] = performance_results
        
        print("âœ… ê³ ê¸‰ ì„±ëŠ¥ ì˜ˆì¸¡ ì™„ë£Œ\n")
    
    def verify_optimization_features(self):
        """ìµœì í™” ê¸°ëŠ¥ ê²€ì¦"""
        print("7. ìµœì í™” ê¸°ëŠ¥ ê²€ì¦ ì¤‘...")
        
        optimization_features = {}
        
        # PyTorch ì»´íŒŒì¼ ì§€ì›
        optimization_features["torch_compile"] = {
            "available": hasattr(torch, 'compile'),
            "recommended": torch.__version__ >= "2.0.0"
        }
        
        # Mixed Precision ì§€ì›
        optimization_features["mixed_precision"] = {
            "available": hasattr(torch.cuda.amp, 'autocast'),
            "tensor_cores": self.check_results["environment"].get("tensor_cores", False)
        }
        
        # Flash Attention í™•ì¸
        try:
            import flash_attn
            optimization_features["flash_attention"] = {"available": True, "version": flash_attn.__version__}
        except ImportError:
            optimization_features["flash_attention"] = {"available": False}
        
        # SDPA ì§€ì› (PyTorch 2.0+)
        optimization_features["sdpa"] = {
            "available": hasattr(torch.nn.functional, 'scaled_dot_product_attention')
        }
        
        # ë³‘ë ¬ ì²˜ë¦¬ ê¸°ëŠ¥
        optimization_features["parallel_processing"] = {
            "cpu_cores": psutil.cpu_count(logical=False),
            "threads": psutil.cpu_count(logical=True),
            "recommended_workers": min(psutil.cpu_count(), 8)
        }
        
        # ìºì‹± ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        optimization_features["caching"] = self._test_caching_performance()
        
        self.check_results["optimization"] = optimization_features
        
        print("âœ… ìµœì í™” ê¸°ëŠ¥ ê²€ì¦ ì™„ë£Œ\n")
    
    def _test_caching_performance(self) -> Dict:
        """ìºì‹± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        cache_test = {}
        
        try:
            # ê°„ë‹¨í•œ ìºì‹± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            test_data = {f"key_{i}": f"value_{i}" for i in range(1000)}
            
            # ë”•ì…”ë„ˆë¦¬ ìºì‹œ í…ŒìŠ¤íŠ¸
            start_time = time.time()
            for key in test_data:
                _ = test_data[key]
            dict_time = time.time() - start_time
            
            cache_test["dict_cache_ms"] = round(dict_time * 1000, 2)
            cache_test["operations_per_second"] = round(1000 / dict_time, 0)
            cache_test["performance"] = "High" if dict_time < 0.001 else "Medium" if dict_time < 0.01 else "Low"
            
        except Exception as e:
            cache_test["error"] = str(e)
        
        return cache_test
    
    def check_compliance_advanced(self):
        """ê³ ê¸‰ ëŒ€íšŒ ê·œì • ì¤€ìˆ˜ ì²´í¬"""
        print("8. ê³ ê¸‰ ëŒ€íšŒ ê·œì • ì¤€ìˆ˜ ê²€ì‚¬ ì¤‘...")
        
        compliance = {}
        
        # ë‹¨ì¼ ëª¨ë¸ ì‚¬ìš© í™•ì¸
        compliance["single_model"] = {
            "status": "âœ… SOLAR-10.7B-Instruct-v1.0 ë‹¨ì¼ ëª¨ë¸ ì‚¬ìš©",
            "verified": True
        }
        
        # ì™¸ë¶€ API ì‚¬ìš© ê²€ì‚¬ (ë” ì—„ê²©í•œ ê²€ì‚¬)
        api_keywords = [
            "requests.get", "requests.post", "urllib.request", "http.client",
            "api_key", "openai", "anthropic", "google.api", "azure.openai",
            "huggingface_hub.login", "api.call", "rest_api"
        ]
        
        api_violations = []
        for py_file in Path(".").glob("*.py"):
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    for keyword in api_keywords:
                        if keyword in content and "#" not in content.split(keyword)[0].split('\n')[-1]:
                            api_violations.append(f"{py_file.name}: {keyword}")
            except:
                continue
        
        compliance["no_external_api"] = {
            "status": "âœ… ì™¸ë¶€ API ì‚¬ìš© ì—†ìŒ" if not api_violations else "âš ï¸ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ API ì½”ë“œ ë°œê²¬",
            "violations": api_violations
        }
        
        # ì•™ìƒë¸” ë°©ë²• ê²€ì‚¬
        ensemble_keywords = [
            "ensemble", "voting", "blend", "stack", "average_predictions",
            "model_fusion", "consensus", "multiple_models"
        ]
        
        ensemble_violations = []
        for py_file in Path(".").glob("*.py"):
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read().lower()
                    
                    for keyword in ensemble_keywords:
                        if keyword in content:
                            # ì»¨í…ìŠ¤íŠ¸ í™•ì¸
                            lines = content.split('\n')
                            for i, line in enumerate(lines):
                                if keyword in line and "#" not in line:
                                    ensemble_violations.append(f"{py_file.name}:{i+1}: {keyword}")
            except:
                continue
        
        compliance["no_ensemble"] = {
            "status": "âœ… ì•™ìƒë¸” ë°©ë²• ì—†ìŒ" if not ensemble_violations else "âš ï¸ ì•™ìƒë¸” ê´€ë ¨ ì½”ë“œ ë°œê²¬",
            "violations": ensemble_violations
        }
        
        # ìƒì„±í˜• AI ì‚¬ìš© í™•ì¸
        generative_features = [
            "AutoModelForCausalLM", "generate", "text-generation",
            "transformer", "llm", "language_model"
        ]
        
        generative_usage = []
        for py_file in Path(".").glob("*.py"):
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    for feature in generative_features:
                        if feature in content:
                            generative_usage.append(feature)
            except:
                continue
        
        compliance["generative_ai_usage"] = {
            "status": "âœ… ìƒì„±í˜• AI ì‚¬ìš© í™•ì¸ë¨",
            "features_found": list(set(generative_usage))
        }
        
        # ë©”ëª¨ë¦¬ ë° ì‹œê°„ ì œí•œ ì¤€ìˆ˜
        performance = self.check_results.get("performance", {})
        compliance["resource_limits"] = {
            "time_compliance": performance.get("time_safety", False),
            "memory_compliance": performance.get("memory_requirements", {}).get("memory_safety", False),
            "estimated_time_min": performance.get("total_processing_time_min", "Unknown"),
            "time_limit_min": 270
        }
        
        self.check_results["compliance"] = compliance
        
        print("âœ… ê³ ê¸‰ ëŒ€íšŒ ê·œì • ì¤€ìˆ˜ ê²€ì‚¬ ì™„ë£Œ\n")
    
    def generate_comprehensive_report(self):
        """í¬ê´„ì  ë³´ê³ ì„œ ìƒì„±"""
        print("\n" + "="*60)
        print("ê³ ì„±ëŠ¥ ì‹œìŠ¤í…œ ê²€ì¦ ì¢…í•© ë³´ê³ ì„œ")
        print("="*60)
        
        total_check_time = time.time() - self.start_time
        
        # ì‹œìŠ¤í…œ ìš”ì•½
        print(f"\nğŸ“Š ì‹œìŠ¤í…œ ìš”ì•½")
        print(f"ê²€ì¦ ì‹œê°„: {total_check_time:.1f}ì´ˆ")
        print(f"í”Œë«í¼: {self.check_results['environment']['platform']}")
        print(f"Python: {self.check_results['environment']['python_version']}")
        print(f"PyTorch: {self.check_results['environment']['pytorch_version']}")
        
        # GPU ë¶„ì„ ìš”ì•½
        if "gpu_0" in self.check_results["environment"]:
            gpu_info = self.check_results["environment"]["gpu_0"]
            print(f"\nğŸš€ GPU ë¶„ì„")
            print(f"GPU: {gpu_info['name']}")
            print(f"ë©”ëª¨ë¦¬: {gpu_info['memory_gb']:.1f}GB")
            print(f"ì„±ëŠ¥ ë“±ê¸‰: {gpu_info['performance_tier']}")
            print(f"ê¶Œì¥ ëª¨ë“œ: {gpu_info['recommended_mode']}")
        
        # íŒŒì¼ ìƒíƒœ
        print(f"\nğŸ“ íŒŒì¼ ìƒíƒœ")
        all_files_ok = self.check_results["files"]["all_present"]
        if all_files_ok:
            print("âœ… ëª¨ë“  í•„ìˆ˜ íŒŒì¼ ì¡´ì¬")
            print(f"ì´ íŒŒì¼ í¬ê¸°: {self.check_results['files']['total_size_mb']:.1f}MB")
        else:
            print("âŒ ì¼ë¶€ íŒŒì¼ ëˆ„ë½")
        
        # ì„±ëŠ¥ ì˜ˆì¸¡
        print(f"\nâš¡ ì„±ëŠ¥ ì˜ˆì¸¡")
        performance = self.check_results.get("performance", {})
        if performance:
            print(f"ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„: {performance['total_processing_time_min']}ë¶„")
            print(f"ì œí•œ ì‹œê°„: {performance['time_limit_min']}ë¶„")
            print(f"ì•ˆì „ ë§ˆì§„: {performance['safety_margin_min']}ë¶„")
            print(f"ì²˜ë¦¬ ì†ë„: {performance['estimated_questions_per_minute']}ë¬¸í•­/ë¶„")
            
            time_safe = performance.get("time_safety", False)
            memory_safe = performance.get("memory_requirements", {}).get("memory_safety", False)
            
            print(f"ì‹œê°„ ì—¬ìœ : {'âœ… ì¶©ë¶„' if time_safe else 'âš ï¸ ë¶€ì¡±'}")
            print(f"ë©”ëª¨ë¦¬ ì—¬ìœ : {'âœ… ì¶©ë¶„' if memory_safe else 'âš ï¸ ë¶€ì¡±'}")
        
        # ìµœì í™” ê¸°ëŠ¥
        print(f"\nğŸ”§ ìµœì í™” ê¸°ëŠ¥")
        optimization = self.check_results.get("optimization", {})
        if optimization:
            print(f"Torch Compile: {'âœ… ì§€ì›' if optimization.get('torch_compile', {}).get('available') else 'âŒ ë¯¸ì§€ì›'}")
            print(f"Mixed Precision: {'âœ… ì§€ì›' if optimization.get('mixed_precision', {}).get('available') else 'âŒ ë¯¸ì§€ì›'}")
            print(f"Flash Attention: {'âœ… ì§€ì›' if optimization.get('flash_attention', {}).get('available') else 'âŒ ë¯¸ì§€ì›'}")
            print(f"SDPA: {'âœ… ì§€ì›' if optimization.get('sdpa', {}).get('available') else 'âŒ ë¯¸ì§€ì›'}")
        
        # ê·œì • ì¤€ìˆ˜
        print(f"\nğŸ“‹ ëŒ€íšŒ ê·œì • ì¤€ìˆ˜")
        compliance = self.check_results.get("compliance", {})
        if compliance:
            for check_name, check_info in compliance.items():
                if isinstance(check_info, dict) and "status" in check_info:
                    print(f"  {check_info['status']}")
        
        # ìµœì¢… íŒì •
        print(f"\n" + "="*60)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        scores = {
            "files": 100 if all_files_ok else 70,
            "performance": 100 if (time_safe and memory_safe) else 80 if time_safe else 60,
            "optimization": 100 if optimization.get("torch_compile", {}).get("available") else 80,
            "compliance": 100 if all(not check.get("violations", []) for check in compliance.values() if isinstance(check, dict)) else 90
        }
        
        overall_score = sum(scores.values()) / len(scores)
        
        print(f"ğŸ¯ ì¢…í•© ì ìˆ˜: {overall_score:.0f}/100")
        
        if overall_score >= 90:
            print("ğŸ† ìš°ìˆ˜: ì‹œìŠ¤í…œì´ ìµœì  ìƒíƒœì…ë‹ˆë‹¤!")
            print("ğŸ’¡ ì‹¤í–‰ ëª…ë ¹: python inference.py")
        elif overall_score >= 80:
            print("âœ… ì–‘í˜¸: ì‹œìŠ¤í…œ ì¤€ë¹„ê°€ ì˜ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ì‹¤í–‰ ëª…ë ¹: python inference.py")
        elif overall_score >= 70:
            print("âš ï¸ ì£¼ì˜: ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            print("âŒ ë¶ˆëŸ‰: ì‹œìŠ¤í…œ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì„±ëŠ¥ í–¥ìƒ ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ ì„±ëŠ¥ í–¥ìƒ ê¶Œì¥ì‚¬í•­:")
        recommendations = self._generate_performance_recommendations()
        for rec in recommendations:
            print(f"  â€¢ {rec}")
        
        print("="*60)
    
    def _generate_performance_recommendations(self) -> List[str]:
        """ì„±ëŠ¥ í–¥ìƒ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # GPU ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if "gpu_0" in self.check_results["environment"]:
            gpu_info = self.check_results["environment"]["gpu_0"]
            tier = gpu_info["performance_tier"]
            
            if tier == "Ultra High":
                recommendations.append("ìµœê³  ì„±ëŠ¥ GPU ê°ì§€ - ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ í™œì„±í™”")
                recommendations.append("Torch Compile ë° Flash Attention ì‚¬ìš© ê¶Œì¥")
            elif tier == "High":
                recommendations.append("ê³ ì„±ëŠ¥ GPU ê°ì§€ - ì ê·¹ì ì¸ ìµœì í™” í™œì„±í™”")
            elif tier == "Medium":
                recommendations.append("ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬ ëª¨ë“œ ì‚¬ìš© ê¶Œì¥")
            else:
                recommendations.append("ì œí•œëœ ì„±ëŠ¥ - ë³´ìˆ˜ì  ì„¤ì • ì‚¬ìš©")
        
        # ìµœì í™” ê¸°ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        optimization = self.check_results.get("optimization", {})
        if not optimization.get("flash_attention", {}).get("available"):
            recommendations.append("Flash Attention ì„¤ì¹˜ ê¶Œì¥ (pip install flash-attn)")
        
        if not optimization.get("torch_compile", {}).get("available"):
            recommendations.append("PyTorch 2.0+ ì—…ê·¸ë ˆì´ë“œ ê¶Œì¥ (torch.compile ì§€ì›)")
        
        # ë©”ëª¨ë¦¬ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        memory_info = self.check_results.get("memory", {})
        if memory_info.get("system_memory", {}).get("usage_percent", 0) > 80:
            recommendations.append("ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ - ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬")
        
        return recommendations

def run_mini_inference_test(sample_size: int = 3):
    """ì†Œê·œëª¨ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ§ª ì†Œê·œëª¨ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ({sample_size}ê°œ ìƒ˜í”Œ)...")
    
    try:
        start_time = time.time()
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        result = subprocess.run(
            [sys.executable, "main.py", "--test-type", "speed", "--sample-size", str(sample_size)],
            capture_output=True,
            text=True,
            timeout=180  # 3ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        
        elapsed = time.time() - start_time
        
        if result.returncode == 0:
            print(f"âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ ({elapsed:.1f}ì´ˆ)")
            print(f"ì˜ˆìƒ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {(elapsed/sample_size*515)/60:.1f}ë¶„")
            
            # ì¶œë ¥ì—ì„œ ìœ ìš©í•œ ì •ë³´ ì¶”ì¶œ
            output_lines = result.stdout.split('\n')[-10:]
            print("ìµœê·¼ ì¶œë ¥:")
            for line in output_lines:
                if line.strip():
                    print(f"  {line.strip()}")
                    
        else:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            print("ì˜¤ë¥˜ ì •ë³´:")
            error_lines = result.stderr.split('\n')[-5:]
            for line in error_lines:
                if line.strip():
                    print(f"  {line.strip()}")
            
    except subprocess.TimeoutExpired:
        print("âŒ í…ŒìŠ¤íŠ¸ íƒ€ì„ì•„ì›ƒ (3ë¶„ ì´ˆê³¼)")
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ê³ ì„±ëŠ¥ ê¸ˆìœµ AI Challenge ì‹œìŠ¤í…œ ê²€ì¦\n")
    
    # í¬ê´„ì  ì‹œìŠ¤í…œ ì²´í¬
    checker = UltraHighPerformanceSystemChecker()
    checker.run_comprehensive_checks()
    
    # ì„ íƒì  ë¯¸ë‹ˆ í…ŒìŠ¤íŠ¸
    response = input("\nì†Œê·œëª¨ ì¶”ë¡  í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
    if response.lower() == 'y':
        run_mini_inference_test(3)
    
    print("\nğŸ‰ ê³ ì„±ëŠ¥ ì‹œìŠ¤í…œ ê²€ì¦ ì™„ë£Œ!")

if __name__ == "__main__":
    main()