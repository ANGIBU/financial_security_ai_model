# test_runner.py

"""
í†µí•© ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°
- 50ë¬¸í•­ ë”¥ëŸ¬ë‹ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- ì‹¤ì œ GPU ì¶”ë¡  ë° í•™ìŠµ ì‹œìŠ¤í…œ ì—°ë™
- íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì§€ì›
- ìƒì„¸í•œ ì„±ëŠ¥ ê²€ì¦ ë° ë¶„ì„
- ë…¼ë¦¬ì  ì¶”ë¡  ì„±ëŠ¥ ì¸¡ì •
- CoT ì¶”ë¡  ê³¼ì • ê²€ì¦
- ì¶”ë¡  í’ˆì§ˆ í‰ê°€ ë©”íŠ¸ë¦­
- í†µí•©ëœ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ë¶„ì„
- ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§
- ë”¥ëŸ¬ë‹ í•™ìŠµ ê³¼ì • ì¶”ì 
"""

import os
import sys
import time
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, Optional, Tuple, List
import threading
import queue

current_dir = Path(__file__).parent.absolute()
sys.path.append(str(current_dir))

from inference import FinancialAIInference

DEFAULT_TEST_SIZE = 50
MAX_TEST_SIZE = 500
MIN_TEST_SIZE = 1

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìƒìˆ˜
PROGRESS_UPDATE_INTERVAL = 5
DETAILED_ANALYSIS_INTERVAL = 10
MEMORY_CHECK_INTERVAL = 20
PERFORMANCE_SNAPSHOT_INTERVAL = 15

class IntegratedTestRunner:
    
    def __init__(self, test_size: int = DEFAULT_TEST_SIZE, use_finetuned: bool = False, 
                 enable_detailed_monitoring: bool = True):
        """í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸° ì´ˆê¸°í™”"""
        self.test_size = max(MIN_TEST_SIZE, min(test_size, MAX_TEST_SIZE))
        self.use_finetuned = use_finetuned
        self.enable_detailed_monitoring = enable_detailed_monitoring
        self.start_time = time.time()
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
        self.progress_queue = queue.Queue()
        self.performance_snapshots = []
        self.current_question_stats = {}
        
        print(f"í†µí•© ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸° ì´ˆê¸°í™” ì¤‘... (ëŒ€ìƒ: {self.test_size}ë¬¸í•­)")
        print(f"ìƒì„¸ ëª¨ë‹ˆí„°ë§: {'í™œì„±í™”' if enable_detailed_monitoring else 'ë¹„í™œì„±í™”'}")
        
        # íŒŒì¸íŠœë‹ ëª¨ë¸ ê²½ë¡œ í™•ì¸
        if use_finetuned and not os.path.exists("./finetuned_model"):
            print("íŒŒì¸íŠœë‹ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.use_finetuned = False
        
        # inference.pyì˜ FinancialAIInference ì‚¬ìš© (í†µí•© ì¶”ë¡  ê¸°ëŠ¥ í¬í•¨)
        try:
            print("í†µí•© ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
            self.inference_engine = FinancialAIInference(
                enable_learning=True,
                verbose=False,  # í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ê°„ê²°í•œ ì¶œë ¥
                use_finetuned=self.use_finetuned
            )
            print("í†µí•© ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            raise RuntimeError(f"í†µí•© ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        model_type = "íŒŒì¸íŠœë‹ëœ ëª¨ë¸" if self.use_finetuned else "ê¸°ë³¸ ëª¨ë¸"
        reasoning_status = "í™œì„±í™”" if self.inference_engine.reasoning_engine else "ë¹„í™œì„±í™”"
        print(f"ì´ˆê¸°í™” ì™„ë£Œ - {model_type} ì‚¬ìš©, ì¶”ë¡  ì—”ì§„: {reasoning_status}\n")
    
    def load_test_data(self, test_file: str, submission_file: str) -> Optional[Tuple[pd.DataFrame, pd.DataFrame]]:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë° ì‚¬ì „ ë¶„ì„"""
        try:
            if not os.path.exists(test_file):
                print(f"ì˜¤ë¥˜: {test_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            if not os.path.exists(submission_file):
                print(f"ì˜¤ë¥˜: {submission_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            print("ë°ì´í„° ë¡œë“œ ë° ì‚¬ì „ ë¶„ì„ ì¤‘...")
            test_df = pd.read_csv(test_file, encoding='utf-8')
            submission_df = pd.read_csv(submission_file, encoding='utf-8')
            
            if len(test_df) < self.test_size:
                print(f"ê²½ê³ : ì „ì²´ {len(test_df)}ë¬¸í•­, ìš”ì²­ {self.test_size}ë¬¸í•­")
                self.test_size = len(test_df)
            
            test_sample = test_df.head(self.test_size).copy()
            submission_sample = submission_df.head(self.test_size).copy()
            
            print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(test_sample)}ë¬¸í•­")
            
            # ë¬¸ì œ ì‚¬ì „ ë¶„ì„
            if self.enable_detailed_monitoring:
                self._preanalyze_questions(test_sample)
            
            return test_sample, submission_sample
            
        except Exception as e:
            print(f"ì˜¤ë¥˜: ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ - {e}")
            return None
    
    def _preanalyze_questions(self, test_df: pd.DataFrame) -> None:
        """ë¬¸ì œ ì‚¬ì „ ë¶„ì„ (ë³µì¡ë„ ë° ì˜ˆìƒ ì²˜ë¦¬ì‹œê°„ ê³„ì‚°)"""
        print("ë¬¸ì œ ì‚¬ì „ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        
        complexity_scores = []
        estimated_times = []
        question_types = {"multiple_choice": 0, "subjective": 0}
        
        for idx, row in test_df.iterrows():
            try:
                question = row['Question']
                
                # êµ¬ì¡° ë¶„ì„ (ì‹¤ì œ ë”¥ëŸ¬ë‹ ë¶„ì„ ìˆ˜í–‰)
                structure = self.inference_engine.data_processor.analyze_question_structure(question)
                
                complexity = structure.get("complexity_score", 0.5)
                complexity_scores.append(complexity)
                
                # ì˜ˆìƒ ì²˜ë¦¬ì‹œê°„ ê³„ì‚°
                base_time = 8.0 if structure["question_type"] == "multiple_choice" else 15.0
                estimated_time = base_time * (1 + complexity)
                estimated_times.append(estimated_time)
                
                question_types[structure["question_type"]] += 1
                
            except Exception as e:
                print(f"ë¬¸ì œ {idx} ë¶„ì„ ì˜¤ë¥˜: {e}")
                complexity_scores.append(0.5)
                estimated_times.append(10.0)
        
        avg_complexity = np.mean(complexity_scores)
        total_estimated_time = sum(estimated_times)
        
        print(f"ì‚¬ì „ ë¶„ì„ ì™„ë£Œ:")
        print(f"  - ê°ê´€ì‹: {question_types['multiple_choice']}ê°œ")
        print(f"  - ì£¼ê´€ì‹: {question_types['subjective']}ê°œ")
        print(f"  - í‰ê·  ë³µì¡ë„: {avg_complexity:.2f}")
        print(f"  - ì˜ˆìƒ ì²˜ë¦¬ì‹œê°„: {total_estimated_time/60:.1f}ë¶„")
        print(f"  - ë¬¸í•­ë‹¹ í‰ê· : {total_estimated_time/self.test_size:.1f}ì´ˆ")
    
    def run_integrated_test(self, test_file: str = "./test.csv", 
                          submission_file: str = "./sample_submission.csv") -> None:
        """í†µí•© ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("="*60)
        print(f"í†µí•© ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹œì‘ ({self.test_size}ë¬¸í•­)")
        if self.use_finetuned:
            print("íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì‚¬ìš©")
        print("ì¶”ë¡  ì—”ì§„, í•™ìŠµ ì‹œìŠ¤í…œ, CoT í”„ë¡¬í”„íŠ¸ ëª¨ë‘ í™œì„±í™”")
        print("="*60)
        
        # ë°ì´í„° ë¡œë“œ
        data_result = self.load_test_data(test_file, submission_file)
        if data_result is None:
            return
        
        test_df, submission_df = data_result
        
        print(f"\ní†µí•© ë”¥ëŸ¬ë‹ ì¶”ë¡  ì‹œì‘...")
        print("ì‹¤ì œ GPU ì¶”ë¡ , CoT ìƒì„±, í•™ìŠµ ì—…ë°ì´íŠ¸ ëª¨ë‘ í™œì„±í™”")
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘
        if self.enable_detailed_monitoring:
            monitor_thread = threading.Thread(target=self._performance_monitor, daemon=True)
            monitor_thread.start()
        
        # ë‹µë³€ ìƒì„± - í†µí•© ì¶”ë¡  ì‹œìŠ¤í…œ ì‚¬ìš©
        answers = []
        detailed_results = []
        
        try:
            for idx, row in test_df.iterrows():
                question_start_time = time.time()
                question = row['Question']
                question_id = row['ID']
                
                print(f"\në¬¸í•­ {idx+1}/{self.test_size}: í†µí•© ì¶”ë¡  ìˆ˜í–‰ ì¤‘...")
                
                # inference.pyì˜ í†µí•© ì¶”ë¡  ë©”ì„œë“œ ì‚¬ìš©
                answer = self.inference_engine.process_question(question, question_id, idx)
                answers.append(answer)
                
                question_processing_time = time.time() - question_start_time
                
                # ìƒì„¸ ê²°ê³¼ ìˆ˜ì§‘
                if self.enable_detailed_monitoring:
                    detailed_result = self._collect_detailed_result(
                        idx, question, answer, question_processing_time
                    )
                    detailed_results.append(detailed_result)
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                if (idx + 1) % PROGRESS_UPDATE_INTERVAL == 0:
                    self._print_progress_update(idx + 1, detailed_results[-PROGRESS_UPDATE_INTERVAL:])
                
                # ì„±ëŠ¥ ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘
                if (idx + 1) % PERFORMANCE_SNAPSHOT_INTERVAL == 0:
                    self._take_performance_snapshot(idx + 1)
            
            # ê²°ê³¼ ì €ì¥
            submission_df['Answer'] = answers
            
            output_file = f"./integrated_test_result_{self.test_size}.csv"
            submission_df.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            # ìƒì„¸ ê²°ê³¼ ë¶„ì„ ë° ì¶œë ¥
            self._print_comprehensive_results(output_file, detailed_results)
            
        except KeyboardInterrupt:
            print("\ní…ŒìŠ¤íŠ¸ ì¤‘ë‹¨ë¨")
        except Exception as e:
            print(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def _collect_detailed_result(self, idx: int, question: str, answer: str, 
                               processing_time: float) -> Dict:
        """ìƒì„¸ ê²°ê³¼ ìˆ˜ì§‘"""
        stats = self.inference_engine.stats
        
        # í˜„ì¬ ë¬¸í•­ì˜ í†µê³„
        detailed_result = {
            "question_idx": idx,
            "question_preview": question[:50] + "..." if len(question) > 50 else question,
            "answer": answer,
            "processing_time": processing_time,
            "model_success": stats.get("model_generation_success", 0) > 0,
            "reasoning_used": stats.get("reasoning_engine_usage", 0) > 0,
            "cot_used": stats.get("cot_prompts_used", 0) > 0,
            "learning_updated": stats.get("learned", 0) > 0,
            "confidence": "high" if stats.get("high_confidence_answers", 0) > 0 else "normal"
        }
        
        # í•™ìŠµ ì‹œìŠ¤í…œ ì •ë³´
        if self.inference_engine.enable_learning:
            learning_stats = self.inference_engine.learning_system.get_learning_statistics()
            detailed_result.update({
                "deep_learning_active": learning_stats.get("deep_learning_active", False),
                "samples_processed": learning_stats.get("samples_processed", 0),
                "gpu_memory_used": learning_stats.get("gpu_memory_used_gb", 0.0)
            })
        
        return detailed_result
    
    def _print_progress_update(self, current: int, recent_results: List[Dict]) -> None:
        """ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì¶œë ¥"""
        if not recent_results:
            return
        
        progress_pct = (current / self.test_size) * 100
        avg_time = np.mean([r["processing_time"] for r in recent_results])
        
        model_success_rate = np.mean([r["model_success"] for r in recent_results]) * 100
        reasoning_rate = np.mean([r["reasoning_used"] for r in recent_results]) * 100
        cot_rate = np.mean([r["cot_used"] for r in recent_results]) * 100
        
        print(f"  ì§„í–‰: {current}/{self.test_size} ({progress_pct:.1f}%)")
        print(f"  ìµœê·¼ {len(recent_results)}ë¬¸í•­ í‰ê· : {avg_time:.2f}ì´ˆ/ë¬¸í•­")
        print(f"  ëª¨ë¸ì„±ê³µ {model_success_rate:.0f}%, ì¶”ë¡ ì—”ì§„ {reasoning_rate:.0f}%, CoT {cot_rate:.0f}%")
        
        # ì˜ˆìƒ ì™„ë£Œ ì‹œê°„
        if current > 5:  # ì¶©ë¶„í•œ ìƒ˜í”Œ í›„ ì˜ˆì¸¡
            remaining = self.test_size - current
            eta_seconds = remaining * avg_time
            eta_minutes = eta_seconds / 60
            print(f"  ì˜ˆìƒ ì™„ë£Œì‹œê°„: {eta_minutes:.1f}ë¶„ í›„")
    
    def _performance_monitor(self) -> None:
        """ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ)"""
        last_check = time.time()
        
        while True:
            time.sleep(5)  # 5ì´ˆë§ˆë‹¤ ì²´í¬
            
            current_time = time.time()
            if current_time - last_check >= MEMORY_CHECK_INTERVAL:
                # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
                try:
                    import torch
                    if torch.cuda.is_available():
                        memory_used = torch.cuda.memory_allocated() / (1024**3)
                        if memory_used > 14.0:  # 16GBì˜ 87.5% ì´ˆê³¼ì‹œ ê²½ê³ 
                            print(f"  [ê²½ê³ ] GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_used:.1f}GB")
                except:
                    pass
                
                last_check = current_time
    
    def _take_performance_snapshot(self, current_idx: int) -> None:
        """ì„±ëŠ¥ ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘"""
        stats = self.inference_engine.stats
        
        snapshot = {
            "timestamp": time.time(),
            "processed_questions": current_idx,
            "total_time": time.time() - self.start_time,
            "model_success_rate": stats.get("model_generation_success", 0) / max(current_idx, 1),
            "reasoning_usage_rate": stats.get("reasoning_engine_usage", 0) / max(current_idx, 1),
            "cot_usage_rate": stats.get("cot_prompts_used", 0) / max(current_idx, 1),
            "learning_samples": stats.get("learned", 0),
            "avg_processing_time": np.mean(stats.get("processing_times", [1.0]))
        }
        
        self.performance_snapshots.append(snapshot)
    
    def _print_comprehensive_results(self, output_file: str, detailed_results: List[Dict]) -> None:
        """ì¢…í•©ì ì¸ ê²°ê³¼ ë¶„ì„ ë° ì¶œë ¥"""
        total_time = time.time() - self.start_time
        
        print(f"\n" + "="*60)
        print("í†µí•© ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print("="*60)
        
        # ê¸°ë³¸ ì²˜ë¦¬ ì •ë³´
        print(f"ì´ ì²˜ë¦¬ì‹œê°„: {total_time:.1f}ì´ˆ ({total_time/60:.1f}ë¶„)")
        print(f"ë¬¸í•­ë‹¹ í‰ê· : {total_time/self.test_size:.2f}ì´ˆ")
        
        # ëª¨ë¸ ì •ë³´
        model_type = "íŒŒì¸íŠœë‹ëœ ëª¨ë¸" if self.use_finetuned else "ê¸°ë³¸ ëª¨ë¸"
        reasoning_status = "í™œì„±í™”" if self.inference_engine.reasoning_engine else "ë¹„í™œì„±í™”"
        print(f"ì‚¬ìš© ëª¨ë¸: {model_type}, ì¶”ë¡  ì—”ì§„: {reasoning_status}")
        
        # inference.pyì˜ ìƒì„¸ í†µê³„ í™œìš©
        self._print_integrated_statistics()
        
        # ìƒì„¸ ê²°ê³¼ ë¶„ì„
        if detailed_results:
            self._analyze_detailed_results(detailed_results)
        
        # ì„±ëŠ¥ ìŠ¤ëƒ…ìƒ· ë¶„ì„
        if self.performance_snapshots:
            self._analyze_performance_trends()
        
        print(f"\nê²°ê³¼ íŒŒì¼: {output_file}")
        print("="*60)
    
    def _print_integrated_statistics(self) -> None:
        """í†µí•© í†µê³„ ì¶œë ¥ (inference.py í†µê³„ í™œìš©)"""
        stats = self.inference_engine.stats
        
        print(f"\nğŸ”¥ í†µí•© ì¶”ë¡  ì„±ëŠ¥:")
        print(f"  ëª¨ë¸ ìƒì„± ì„±ê³µ: {stats['model_generation_success']}/{stats['total']} ({stats['model_generation_success']/max(stats['total'],1)*100:.1f}%)")
        print(f"  ì¶”ë¡  ì—”ì§„ ì‚¬ìš©: {stats['reasoning_engine_usage']}/{stats['total']} ({stats['reasoning_engine_usage']/max(stats['total'],1)*100:.1f}%)")
        print(f"  CoT í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {stats['cot_prompts_used']}/{stats['total']} ({stats['cot_prompts_used']/max(stats['total'],1)*100:.1f}%)")
        print(f"  ê³ ì‹ ë¢°ë„ ë‹µë³€: {stats['high_confidence_answers']}/{stats['total']} ({stats['high_confidence_answers']/max(stats['total'],1)*100:.1f}%)")
        print(f"  í´ë°± ì‚¬ìš©: {stats['fallback_used']}/{stats['total']} ({stats['fallback_used']/max(stats['total'],1)*100:.1f}%)")
        
        # ì¶”ë¡  ì—”ì§„ ìƒì„¸ í†µê³„
        if self.inference_engine.reasoning_engine:
            print(f"\nğŸ§  ì¶”ë¡  ì—”ì§„ ìƒì„¸:")
            print(f"  ì¶”ë¡  ì„±ê³µ: {stats['reasoning_successful']}íšŒ")
            print(f"  ì¶”ë¡  ì‹¤íŒ¨: {stats['reasoning_failed']}íšŒ")
            print(f"  í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼: {stats['hybrid_approach_used']}íšŒ")
            print(f"  ê²€ì¦ í†µê³¼: {stats['verification_passed']}íšŒ")
            print(f"  ê²€ì¦ ì‹¤íŒ¨: {stats['verification_failed']}íšŒ")
            
            if stats['reasoning_time']:
                avg_reasoning_time = np.mean(stats['reasoning_time'])
                print(f"  í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_reasoning_time:.3f}ì´ˆ")
            
            if stats['reasoning_chain_lengths']:
                avg_chain_length = np.mean(stats['reasoning_chain_lengths'])
                print(f"  í‰ê·  ì¶”ë¡  ì²´ì¸ ê¸¸ì´: {avg_chain_length:.1f}ë‹¨ê³„")
        
        # íŒŒì¸íŠœë‹ ëª¨ë¸ í†µê³„
        if self.use_finetuned:
            finetuned_rate = stats['finetuned_usage'] / max(stats['total'], 1) * 100
            print(f"\nâš¡ íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš©ë¥ : {finetuned_rate:.1f}%")
        
        # í•™ìŠµ ì‹œìŠ¤í…œ í†µê³„
        if self.inference_engine.enable_learning:
            print(f"\nğŸ“š ë”¥ëŸ¬ë‹ í•™ìŠµ ì‹œìŠ¤í…œ:")
            learning_stats = self.inference_engine.learning_system.get_learning_statistics()
            print(f"  í•™ìŠµëœ ìƒ˜í”Œ: {stats['learned']}ê°œ")
            print(f"  ë”¥ëŸ¬ë‹ í™œì„±í™”: {learning_stats['deep_learning_active']}")
            print(f"  ì²˜ë¦¬ëœ ìƒ˜í”Œ: {learning_stats['samples_processed']}ê°œ")
            print(f"  ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸: {learning_stats['weights_updated']}íšŒ")
            print(f"  GPU ë©”ëª¨ë¦¬ ì‚¬ìš©: {learning_stats['gpu_memory_used_gb']:.2f}GB")
            print(f"  ì´ í•™ìŠµ ì‹œê°„: {learning_stats['total_training_time']:.1f}ì´ˆ")
            if learning_stats['average_loss'] > 0:
                print(f"  í‰ê·  ì†ì‹¤: {learning_stats['average_loss']:.4f}")
            print(f"  í˜„ì¬ ì •í™•ë„: {self.inference_engine.learning_system.get_current_accuracy():.2%}")
        
        # í•œêµ­ì–´ í’ˆì§ˆ í†µê³„
        if stats['quality_scores']:
            avg_quality = np.mean(stats['quality_scores'])
            quality_level = "ìš°ìˆ˜" if avg_quality > 0.8 else "ì–‘í˜¸" if avg_quality > 0.65 else "ê°œì„  í•„ìš”"
            print(f"\nğŸ‡°ğŸ‡· í•œêµ­ì–´ í’ˆì§ˆ: {avg_quality:.2f} ({quality_level})")
        
        # ë‹µë³€ ë¶„í¬
        distribution = stats['answer_distribution']
        total_mc = sum(distribution.values())
        if total_mc > 0:
            print(f"\nğŸ“Š ê°ê´€ì‹ ë‹µë³€ ë¶„í¬:")
            for ans in sorted(distribution.keys()):
                count = distribution[ans]
                if count > 0:
                    pct = count / total_mc * 100
                    print(f"  {ans}ë²ˆ: {count}ê°œ ({pct:.1f}%)")
            
            unique_answers = len([k for k, v in distribution.items() if v > 0])
            diversity = "ìš°ìˆ˜" if unique_answers >= 4 else "ì–‘í˜¸" if unique_answers >= 3 else "ê°œì„  í•„ìš”"
            print(f"  ë‹µë³€ ë‹¤ì–‘ì„±: {diversity} ({unique_answers}/5ê°œ ë²ˆí˜¸ ì‚¬ìš©)")
    
    def _analyze_detailed_results(self, detailed_results: List[Dict]) -> None:
        """ìƒì„¸ ê²°ê³¼ ë¶„ì„"""
        if not detailed_results:
            return
        
        print(f"\nğŸ“ˆ ìƒì„¸ ì„±ëŠ¥ ë¶„ì„:")
        
        # ì²˜ë¦¬ì‹œê°„ ë¶„ì„
        processing_times = [r["processing_time"] for r in detailed_results]
        print(f"  ì²˜ë¦¬ì‹œê°„ - ìµœì†Œ: {min(processing_times):.2f}ì´ˆ, ìµœëŒ€: {max(processing_times):.2f}ì´ˆ")
        print(f"  ì²˜ë¦¬ì‹œê°„ - í‰ê· : {np.mean(processing_times):.2f}ì´ˆ, ì¤‘ì•™ê°’: {np.median(processing_times):.2f}ì´ˆ")
        
        # ì„±ê³µë¥  ë¶„ì„
        model_success_rate = np.mean([r["model_success"] for r in detailed_results]) * 100
        reasoning_rate = np.mean([r["reasoning_used"] for r in detailed_results]) * 100
        cot_rate = np.mean([r["cot_used"] for r in detailed_results]) * 100
        learning_rate = np.mean([r["learning_updated"] for r in detailed_results]) * 100
        
        print(f"  ì„±ê³µë¥  - ëª¨ë¸: {model_success_rate:.1f}%, ì¶”ë¡ : {reasoning_rate:.1f}%, CoT: {cot_rate:.1f}%, í•™ìŠµ: {learning_rate:.1f}%")
        
        # ì‹ ë¢°ë„ ë¶„ì„
        high_conf_rate = np.mean([r["confidence"] == "high" for r in detailed_results]) * 100
        print(f"  ê³ ì‹ ë¢°ë„ ë‹µë³€ ë¹„ìœ¨: {high_conf_rate:.1f}%")
        
        # ë”¥ëŸ¬ë‹ í•™ìŠµ ë¶„ì„
        if any("deep_learning_active" in r for r in detailed_results):
            dl_active_rate = np.mean([r.get("deep_learning_active", False) for r in detailed_results]) * 100
            avg_samples = np.mean([r.get("samples_processed", 0) for r in detailed_results])
            avg_gpu_memory = np.mean([r.get("gpu_memory_used", 0.0) for r in detailed_results])
            
            print(f"  ë”¥ëŸ¬ë‹ í™œì„±í™”ìœ¨: {dl_active_rate:.1f}%")
            print(f"  í‰ê·  ì²˜ë¦¬ ìƒ˜í”Œ: {avg_samples:.1f}ê°œ")
            print(f"  í‰ê·  GPU ë©”ëª¨ë¦¬: {avg_gpu_memory:.2f}GB")
    
    def _analyze_performance_trends(self) -> None:
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„"""
        if len(self.performance_snapshots) < 2:
            return
        
        print(f"\nğŸ“Š ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„:")
        
        # ì²˜ë¦¬ì†ë„ íŠ¸ë Œë“œ
        early_snapshot = self.performance_snapshots[0]
        late_snapshot = self.performance_snapshots[-1]
        
        speed_change = late_snapshot["avg_processing_time"] - early_snapshot["avg_processing_time"]
        speed_trend = "í–¥ìƒ" if speed_change < 0 else "ì €í•˜" if speed_change > 0 else "ì•ˆì •"
        
        print(f"  ì²˜ë¦¬ì†ë„ íŠ¸ë Œë“œ: {speed_trend} ({speed_change:+.2f}ì´ˆ)")
        
        # ì„±ê³µë¥  íŠ¸ë Œë“œ
        success_change = late_snapshot["model_success_rate"] - early_snapshot["model_success_rate"]
        success_trend = "í–¥ìƒ" if success_change > 0 else "ì €í•˜" if success_change < 0 else "ì•ˆì •"
        
        print(f"  ëª¨ë¸ ì„±ê³µë¥  íŠ¸ë Œë“œ: {success_trend} ({success_change:+.1%})")
        
        # í•™ìŠµ ì§„í–‰ìƒí™©
        learning_progress = late_snapshot["learning_samples"] - early_snapshot["learning_samples"]
        print(f"  í•™ìŠµ ì§„í–‰: +{learning_progress}ê°œ ìƒ˜í”Œ")
    
    def get_integration_test_summary(self) -> Dict:
        """í†µí•© í…ŒìŠ¤íŠ¸ ìš”ì•½"""
        stats = self.inference_engine.stats
        
        if stats["total"] == 0:
            return {"error": "ì•„ì§ í…ŒìŠ¤íŠ¸ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
        
        summary = {
            "ì´_ë¬¸í•­": stats["total"],
            "ëª¨ë¸_ì„±ê³µë¥ ": f"{stats['model_generation_success']/stats['total']*100:.1f}%",
            "ì¶”ë¡ _ì—”ì§„_ì‚¬ìš©ë¥ ": f"{stats['reasoning_engine_usage']/stats['total']*100:.1f}%",
            "CoT_ì‚¬ìš©ë¥ ": f"{stats['cot_prompts_used']/stats['total']*100:.1f}%",
            "í•™ìŠµ_ìƒ˜í”Œ": stats['learned'],
            "í´ë°±_ì‚¬ìš©ë¥ ": f"{stats['fallback_used']/stats['total']*100:.1f}%",
            "í‰ê· _ì²˜ë¦¬ì‹œê°„": f"{np.mean(stats['processing_times']):.2f}ì´ˆ" if stats['processing_times'] else "N/A"
        }
        
        if self.use_finetuned:
            summary["íŒŒì¸íŠœë‹_ì‚¬ìš©ë¥ "] = f"{stats['finetuned_usage']/stats['total']*100:.1f}%"
        
        if stats["quality_scores"]:
            summary["í•œêµ­ì–´_í’ˆì§ˆ"] = f"{np.mean(stats['quality_scores']):.2f}"
        
        if self.inference_engine.enable_learning:
            learning_stats = self.inference_engine.learning_system.get_learning_statistics()
            summary["ë”¥ëŸ¬ë‹_í™œì„±í™”"] = learning_stats['deep_learning_active']
            summary["GPU_ë©”ëª¨ë¦¬_ì‚¬ìš©"] = f"{learning_stats['gpu_memory_used_gb']:.1f}GB"
            summary["í•™ìŠµ_ì •í™•ë„"] = f"{self.inference_engine.learning_system.get_current_accuracy():.1%}"
        
        return summary
    
    def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            print("\nì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘...")
            
            # inference.pyì˜ cleanup ë©”ì„œë“œ ì‚¬ìš©
            if hasattr(self, 'inference_engine'):
                self.inference_engine.cleanup()
            
            # ì„±ëŠ¥ ë°ì´í„° ì •ë¦¬
            self.performance_snapshots.clear()
            
            print("ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    test_size = DEFAULT_TEST_SIZE
    use_finetuned = False
    enable_monitoring = True
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
    if len(sys.argv) > 1:
        try:
            test_size = int(sys.argv[1])
            test_size = max(MIN_TEST_SIZE, min(test_size, MAX_TEST_SIZE))
        except ValueError:
            print("ì˜ëª»ëœ ë¬¸í•­ ìˆ˜, ê¸°ë³¸ê°’ 50 ì‚¬ìš©")
            test_size = DEFAULT_TEST_SIZE
    
    if len(sys.argv) > 2:
        if sys.argv[2].lower() in ['true', '1', 'yes', 'finetuned']:
            use_finetuned = True
    
    if len(sys.argv) > 3:
        if sys.argv[3].lower() in ['false', '0', 'no', 'simple']:
            enable_monitoring = False
    
    # íŒŒì¸íŠœë‹ ëª¨ë¸ ìë™ ê°ì§€
    if os.path.exists("./finetuned_model") and not use_finetuned:
        try:
            response = input("íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            if response.lower() in ['y', 'yes']:
                use_finetuned = True
        except (EOFError, KeyboardInterrupt):
            print("\nê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
    
    print(f"í†µí•© ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸° ì‹œì‘ (Python {sys.version.split()[0]})")
    print(f"GPU ê¸°ë°˜ ë”¥ëŸ¬ë‹ ì¶”ë¡  ë° í•™ìŠµ ì‹œìŠ¤í…œ í™œì„±í™”")
    
    runner = None
    try:
        runner = IntegratedTestRunner(
            test_size=test_size, 
            use_finetuned=use_finetuned,
            enable_detailed_monitoring=enable_monitoring
        )
        runner.run_integrated_test()
        
        # í†µí•© ì„±ëŠ¥ ìš”ì•½ ì¶œë ¥
        summary = runner.get_integration_test_summary()
        print(f"\nğŸ¯ í†µí•© í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ìš”ì•½:")
        for key, value in summary.items():
            print(f"  {key}: {value}")
        
        # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
        if summary.get("ëª¨ë¸_ì„±ê³µë¥ ", "0%") == "0.0%":
            print(f"\nâš ï¸  ê²½ê³ : ëª¨ë¸ ìƒì„± ì„±ê³µë¥ ì´ 0%ì…ë‹ˆë‹¤. ì‹¤ì œ GPU ì¶”ë¡ ì´ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif float(summary.get("ëª¨ë¸_ì„±ê³µë¥ ", "0%").rstrip("%")) > 70:
            print(f"\nâœ… ì„±ê³µ: í†µí•© ì¶”ë¡  ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
        else:
            print(f"\nâš ï¸  ì£¼ì˜: ëª¨ë¸ ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ì ê²€ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
    except KeyboardInterrupt:
        print("\ní…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if runner:
            runner.cleanup()


if __name__ == "__main__":
    main()