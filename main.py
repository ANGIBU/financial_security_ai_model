# main.py
"""
ê°œë°œ/í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ íŒŒì¼
"""

import os
import pandas as pd
import torch
import time
import argparse
from tqdm import tqdm
import warnings
import numpy as np
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil
warnings.filterwarnings("ignore")

from model_handler import OptimizedModelHandler
from data_processor import IntelligentDataProcessor
from prompt_engineering import AdvancedPromptEngineer
from knowledge_base import FinancialSecurityKnowledgeBase
from advanced_optimizer import UltraHighPerformanceOptimizer, PerformanceMonitor
from pattern_learner import AnswerPatternLearner, SmartAnswerSelector

class UltraHighPerformanceTester:
    """ê°œë°œ ë° í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, model_config: dict):
        print("ê°œë°œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.performance_monitor = PerformanceMonitor()
        self.start_time = time.time()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.model_handler = OptimizedModelHandler(**model_config)
        self.data_processor = IntelligentDataProcessor()
        self.prompt_engineer = AdvancedPromptEngineer()
        self.knowledge_base = FinancialSecurityKnowledgeBase()
        self.optimizer = UltraHighPerformanceOptimizer()
        self.pattern_learner = AnswerPatternLearner()
        self.answer_selector = SmartAnswerSelector()
        
        # ì‹œìŠ¤í…œ ì„±ëŠ¥ ì •ë³´
        self.system_info = self._collect_system_info()
        
        print("âœ… ê³ ì„±ëŠ¥ ì´ˆê¸°í™” ì™„ë£Œ")
        self._print_system_summary()
    
    def _collect_system_info(self) -> dict:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        info = {
            "gpu_memory_gb": 0,
            "cpu_cores": psutil.cpu_count(logical=False),
            "ram_gb": psutil.virtual_memory().total / (1024**3),
            "performance_tier": "Basic"
        }
        
        if torch.cuda.is_available():
            gpu_props = torch.cuda.get_device_properties(0)
            info["gpu_name"] = gpu_props.name
            info["gpu_memory_gb"] = gpu_props.total_memory / (1024**3)
            
            # ì„±ëŠ¥ ë“±ê¸‰ íŒì •
            if info["gpu_memory_gb"] >= 20:
                info["performance_tier"] = "Ultra High"
            elif info["gpu_memory_gb"] >= 12:
                info["performance_tier"] = "High"
            elif info["gpu_memory_gb"] >= 8:
                info["performance_tier"] = "Medium"
        
        return info
    
    def _print_system_summary(self):
        """ì‹œìŠ¤í…œ ìš”ì•½ ì¶œë ¥"""
        print(f"\nğŸ“Š ì‹œìŠ¤í…œ ì •ë³´")
        if torch.cuda.is_available():
            print(f"GPU: {self.system_info['gpu_name']} ({self.system_info['gpu_memory_gb']:.1f}GB)")
        print(f"CPU: {self.system_info['cpu_cores']}ì½”ì–´")
        print(f"RAM: {self.system_info['ram_gb']:.1f}GB")
        print(f"ì„±ëŠ¥ ë“±ê¸‰: {self.system_info['performance_tier']}")
    
    def run_comprehensive_accuracy_test(self, sample_size: int = 30):
        """í¬ê´„ì  ì •í™•ë„ í…ŒìŠ¤íŠ¸"""
        
        test_df = pd.read_csv('./test.csv')
        
        print(f"\nğŸ¯ í¬ê´„ì  ì •í™•ë„ í…ŒìŠ¤íŠ¸: {sample_size}ê°œ ë¬¸í•­")
        
        # ì§€ëŠ¥í˜• ìƒ˜í”Œ ì„ íƒ
        sample_indices = self._select_intelligent_samples(test_df, sample_size)
        
        results = []
        confidence_scores = []
        processing_times = []
        answer_distribution = {"1": 0, "2": 0, "3": 0, "4": 0, "5": 0}
        
        # ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ì²˜ë¦¬
        for idx in tqdm(sample_indices, desc="ì •í™•ë„ í…ŒìŠ¤íŠ¸"):
            start_time = time.time()
            
            question = test_df.iloc[idx]['Question']
            question_id = test_df.iloc[idx]['ID']
            
            # ê³ ê¸‰ ë¬¸ì œ ë¶„ì„
            structure = self.data_processor.analyze_question_structure(question)
            analysis = self.knowledge_base.analyze_question(question)
            difficulty = self.optimizer.evaluate_question_difficulty_advanced(question, structure)
            
            # ì ì‘í˜• ì „ëµ ì„ íƒ
            strategies = self._select_adaptive_strategies(difficulty, structure)
            
            best_answer = None
            best_confidence = 0
            best_reasoning = ""
            
            # ë‹¤ì¤‘ ì „ëµ í…ŒìŠ¤íŠ¸
            for strategy in strategies:
                try:
                    # ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ìƒì„±
                    prompt = self.prompt_engineer.create_adaptive_prompt(
                        question, structure["question_type"], analysis, strategy
                    )
                    
                    # ëª¨ë¸ë³„ ìµœì í™”
                    optimized_prompt = self.prompt_engineer.optimize_for_model(
                        prompt, self.model_handler.model_name
                    )
                    
                    # ì¶”ë¡  ì‹¤í–‰
                    result = self.model_handler.generate_expert_response(
                        optimized_prompt, structure["question_type"], max_attempts=1
                    )
                    
                    # ë‹µë³€ í›„ì²˜ë¦¬
                    processed_answer = self.data_processor.post_process_answer(
                        result.response, question, structure["question_type"]
                    )
                    
                    # ìŠ¤ë§ˆíŠ¸ ì„ íƒê¸°ë¡œ ìµœì¢… ê²°ì •
                    final_answer, final_confidence = self.answer_selector.select_best_answer(
                        question, result.response, structure, result.confidence
                    )
                    
                    if final_confidence > best_confidence:
                        best_confidence = final_confidence
                        best_answer = final_answer
                        best_reasoning = result.response[:200] + "..."
                        
                except Exception as e:
                    continue
            
            processing_time = time.time() - start_time
            processing_times.append(processing_time)
            
            # ê²°ê³¼ ê¸°ë¡
            if best_answer and structure["question_type"] == "multiple_choice":
                if best_answer in answer_distribution:
                    answer_distribution[best_answer] += 1
            
            confidence_scores.append(best_confidence)
            
            results.append({
                "id": question_id,
                "question": question[:100] + "...",
                "answer": best_answer,
                "confidence": best_confidence,
                "reasoning": best_reasoning,
                "type": structure["question_type"],
                "difficulty": difficulty.score,
                "processing_time": processing_time,
                "domain": analysis.get("domain", ["ì¼ë°˜"])
            })
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì—…ë°ì´íŠ¸
            self.performance_monitor.update(processing_time, best_confidence)
        
        # ìƒì„¸ ê²°ê³¼ ë¶„ì„
        self._analyze_comprehensive_accuracy_results(results, confidence_scores, 
                                                    processing_times, answer_distribution)
        
        return results
    
    def run_ultra_speed_test(self, sample_size: int = 100):
        """ì´ˆê³ ì† í…ŒìŠ¤íŠ¸"""
        
        test_df = pd.read_csv('./test.csv')
        
        print(f"\nâš¡ ì´ˆê³ ì† í…ŒìŠ¤íŠ¸: {sample_size}ê°œ ë¬¸í•­")
        
        # ë¹ ë¥¸ ì²˜ë¦¬ìš© ìƒ˜í”Œ ì„ íƒ (ì‰¬ìš´ ë¬¸ì œ ìœ„ì£¼)
        sample_indices = self._select_speed_optimized_samples(test_df, sample_size)
        
        start_time = time.time()
        results = []
        
        # ë™ì  ë°°ì¹˜ í¬ê¸° ê²°ì •
        gpu_memory = self.system_info["gpu_memory_gb"]
        if self.system_info["performance_tier"] == "Ultra High":
            batch_size = 20
        elif self.system_info["performance_tier"] == "High":
            batch_size = 15
        else:
            batch_size = 10
        
        print(f"ğŸ”§ ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        # ë°°ì¹˜ë³„ ë³‘ë ¬ ì²˜ë¦¬
        batch_results = []
        for i in tqdm(range(0, len(sample_indices), batch_size), desc="ë°°ì¹˜ ì²˜ë¦¬"):
            batch_indices = sample_indices[i:i+batch_size]
            batch_questions = [test_df.iloc[idx]['Question'] for idx in batch_indices]
            
            # ë°°ì¹˜ ì²˜ë¦¬
            batch_start = time.time()
            processed_batch = self._process_speed_batch(batch_questions, batch_indices, test_df)
            batch_time = time.time() - batch_start
            
            batch_results.extend(processed_batch)
            
            print(f"  ë°°ì¹˜ {i//batch_size + 1}: {len(processed_batch)}ê°œ, {batch_time:.1f}ì´ˆ")
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬
            if i % (batch_size * 3) == 0:
                torch.cuda.empty_cache()
        
        total_time = time.time() - start_time
        
        # ì†ë„ ë¶„ì„
        self._analyze_speed_results(batch_results, total_time, sample_size)
        
        return batch_results
    
    def run_stress_test(self, duration_minutes: int = 10):
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
        
        print(f"\nğŸ”¥ ì‹œìŠ¤í…œ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸: {duration_minutes}ë¶„")
        
        test_df = pd.read_csv('./test.csv')
        end_time = time.time() + (duration_minutes * 60)
        
        processed_count = 0
        error_count = 0
        performance_history = []
        
        while time.time() < end_time:
            try:
                # ëœë¤ ë¬¸ì œ ì„ íƒ
                idx = np.random.randint(0, len(test_df))
                question = test_df.iloc[idx]['Question']
                
                start = time.time()
                
                # ë¹ ë¥¸ ì²˜ë¦¬
                structure = self.data_processor.analyze_question_structure(question)
                prompt = self.prompt_engineer.create_adaptive_prompt(
                    question, structure["question_type"], {}, "simple"
                )
                
                result = self.model_handler.generate_expert_response(
                    prompt, structure["question_type"], max_attempts=1
                )
                
                processing_time = time.time() - start
                
                # ì„±ëŠ¥ ê¸°ë¡
                performance_history.append({
                    "time": time.time(),
                    "processing_time": processing_time,
                    "confidence": result.confidence,
                    "gpu_memory": torch.cuda.memory_allocated() / (1024**3) if torch.cuda.is_available() else 0
                })
                
                processed_count += 1
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥ (10ì´ˆë§ˆë‹¤)
                if processed_count % 10 == 0:
                    elapsed = time.time() - (end_time - duration_minutes * 60)
                    remaining = (end_time - time.time()) / 60
                    rate = processed_count / (elapsed / 60)
                    print(f"  ì§„í–‰: {processed_count}ê°œ ì²˜ë¦¬, {rate:.1f}ê°œ/ë¶„, ë‚¨ì€ì‹œê°„: {remaining:.1f}ë¶„")
                
            except Exception as e:
                error_count += 1
                if error_count > 10:  # ë„ˆë¬´ ë§ì€ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ë‹¨
                    print(f"âš ï¸ ê³¼ë„í•œ ì˜¤ë¥˜ ë°œìƒ ({error_count}ê°œ), í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
                    break
        
        # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ë¶„ì„
        self._analyze_stress_test_results(processed_count, error_count, performance_history, duration_minutes)
        
        return {
            "processed_count": processed_count,
            "error_count": error_count,
            "performance_history": performance_history
        }
    
    def _select_intelligent_samples(self, test_df: pd.DataFrame, sample_size: int) -> list:
        """ì§€ëŠ¥í˜• ìƒ˜í”Œ ì„ íƒ"""
        indices = []
        
        # ì „ëµì  íŒ¨í„´ë³„ ì„ íƒ
        patterns = [
            ("ê°œì¸ì •ë³´", min(8, sample_size//4)),      # ê°œì¸ì •ë³´ë³´í˜¸ ë¬¸ì œ
            ("ì „ìê¸ˆìœµ", min(8, sample_size//4)),      # ì „ìê¸ˆìœµ ë¬¸ì œ
            ("í•´ë‹¹í•˜ì§€ì•ŠëŠ”", min(6, sample_size//5)),   # ë¶€ì •í˜• ë¬¸ì œ
            ("ì •ì˜", min(4, sample_size//6)),          # ì •ì˜ ë¬¸ì œ
            ("ë²•", min(4, sample_size//6))            # ë²•ë ¹ ë¬¸ì œ
        ]
        
        used_indices = set()
        
        # íŒ¨í„´ë³„ ìƒ˜í”Œë§
        for pattern, target_count in patterns:
            found_count = 0
            for i, question in enumerate(test_df['Question']):
                if pattern in question and i not in used_indices:
                    # ë¬¸ì œ í’ˆì§ˆ ê²€ì‚¬
                    if self._is_good_sample(question):
                        indices.append(i)
                        used_indices.add(i)
                        found_count += 1
                        if found_count >= target_count:
                            break
        
        # ëœë¤ ìƒ˜í”Œë¡œ ë¶€ì¡±í•œ ë¶€ë¶„ ì±„ìš°ê¸°
        remaining = sample_size - len(indices)
        if remaining > 0:
            available_indices = [i for i in range(len(test_df)) if i not in used_indices]
            random_indices = np.random.choice(available_indices, 
                                            min(remaining, len(available_indices)), 
                                            replace=False)
            indices.extend(random_indices)
        
        return indices[:sample_size]
    
    def _select_speed_optimized_samples(self, test_df: pd.DataFrame, sample_size: int) -> list:
        """ì†ë„ ìµœì í™”ëœ ìƒ˜í”Œ ì„ íƒ"""
        # ì§§ê³  ê°„ë‹¨í•œ ë¬¸ì œ ìœ„ì£¼ ì„ íƒ
        simple_indices = []
        
        for i, question in enumerate(test_df['Question']):
            if len(question) < 300:  # ì§§ì€ ë¬¸ì œ
                if not any(neg in question for neg in ["í•´ë‹¹í•˜ì§€ì•ŠëŠ”", "ì ì ˆí•˜ì§€ì•Šì€"]):  # ë¶€ì •í˜• ì œì™¸
                    simple_indices.append(i)
        
        # ìƒ˜í”Œ í¬ê¸°ë§Œí¼ ëœë¤ ì„ íƒ
        if len(simple_indices) >= sample_size:
            return np.random.choice(simple_indices, sample_size, replace=False).tolist()
        else:
            # ë¶€ì¡±í•˜ë©´ ì „ì²´ì—ì„œ ëœë¤ ì„ íƒ
            additional = sample_size - len(simple_indices)
            remaining_indices = [i for i in range(len(test_df)) if i not in simple_indices]
            additional_indices = np.random.choice(remaining_indices, additional, replace=False)
            return simple_indices + additional_indices.tolist()
    
    def _is_good_sample(self, question: str) -> bool:
        """ì¢‹ì€ ìƒ˜í”Œì¸ì§€ íŒë‹¨"""
        # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë¬¸ì œ ì œì™¸
        if len(question) < 50 or len(question) > 2000:
            return False
        
        # íŠ¹ìˆ˜ë¬¸ìê°€ ë„ˆë¬´ ë§ì€ ë¬¸ì œ ì œì™¸
        special_char_ratio = len(re.findall(r'[^\w\sê°€-í£]', question)) / len(question)
        if special_char_ratio > 0.3:
            return False
        
        return True
    
    def _select_adaptive_strategies(self, difficulty, structure) -> list:
        """ì ì‘í˜• ì „ëµ ì„ íƒ"""
        strategies = []
        
        # ë‚œì´ë„ ê¸°ë°˜ ì „ëµ
        if difficulty.score < 0.3:
            strategies = ["simple"]
        elif difficulty.score < 0.6:
            strategies = ["simple", "balanced"]
        else:
            strategies = ["balanced", "comprehensive"]
        
        # êµ¬ì¡° ê¸°ë°˜ ì¶”ê°€ ì „ëµ
        if structure.get("has_negative", False):
            strategies.append("negative_focused")
        
        # ë„ë©”ì¸ ê¸°ë°˜ ì „ëµ
        domains = structure.get("domain", [])
        if domains and any(d in ["ê°œì¸ì •ë³´ë³´í˜¸", "ì „ìê¸ˆìœµ"] for d in domains):
            strategies.append("domain_specific")
        
        return strategies[:2]  # ìµœëŒ€ 2ê°œ ì „ëµ
    
    def _process_speed_batch(self, questions: list, indices: list, test_df: pd.DataFrame) -> list:
        """ì†ë„ ìµœì í™” ë°°ì¹˜ ì²˜ë¦¬"""
        batch_results = []
        
        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë¹ ë¥¸ ì²˜ë¦¬
        for i, question in enumerate(questions):
            try:
                # ìµœì†Œí•œì˜ ë¶„ì„
                is_mc = bool(re.search(r'[â‘ â‘¡â‘¢â‘£â‘¤]|\b[1-5]\s*[.)]', question))
                
                # ë¹ ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
                if is_mc:
                    prompt = f"ë‹¤ìŒ ê°ê´€ì‹ ë¬¸ì œì˜ ì •ë‹µ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n\n{question}\n\nì •ë‹µ:"
                else:
                    prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n\n{question}\n\në‹µë³€:"
                
                # ëª¨ë¸ ìµœì í™”
                optimized_prompt = self.prompt_engineer.optimize_for_model(
                    prompt, self.model_handler.model_name
                )
                
                # ë¹ ë¥¸ ìƒì„± (íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)
                result = self.model_handler.generate_expert_response(
                    optimized_prompt, "multiple_choice" if is_mc else "subjective", 
                    max_attempts=1
                )
                
                # ë¹ ë¥¸ ë‹µë³€ ì¶”ì¶œ
                if is_mc:
                    answer = self.data_processor.extract_mc_answer_fast(result.response)
                else:
                    answer = result.response[:200]  # ì£¼ê´€ì‹ì€ 200ìë¡œ ì œí•œ
                
                batch_results.append({
                    "id": test_df.iloc[indices[i]]['ID'],
                    "answer": answer,
                    "confidence": result.confidence,
                    "time": result.inference_time
                })
                
            except Exception as e:
                # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’
                batch_results.append({
                    "id": test_df.iloc[indices[i]]['ID'],
                    "answer": "3" if is_mc else "ê´€ë ¨ ê·œì •ì— ë”°ë¥¸ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                    "confidence": 0.3,
                    "time": 0.1
                })
        
        return batch_results
    
    def _analyze_comprehensive_accuracy_results(self, results: list, confidence_scores: list, 
                                              processing_times: list, answer_distribution: dict):
        """í¬ê´„ì  ì •í™•ë„ ê²°ê³¼ ë¶„ì„"""
        
        print(f"\nğŸ¯ ì •í™•ë„ ë¶„ì„ ê²°ê³¼")
        print(f"{'='*50}")
        
        # ê¸°ë³¸ í†µê³„
        mc_results = [r for r in results if r["type"] == "multiple_choice"]
        subj_results = [r for r in results if r["type"] == "subjective"]
        
        print(f"ì´ ë¬¸í•­: {len(results)}ê°œ")
        print(f"  ê°ê´€ì‹: {len(mc_results)}ê°œ ({len(mc_results)/len(results)*100:.1f}%)")
        print(f"  ì£¼ê´€ì‹: {len(subj_results)}ê°œ ({len(subj_results)/len(results)*100:.1f}%)")
        
        # ì‹ ë¢°ë„ ë¶„ì„
        if confidence_scores:
            avg_confidence = np.mean(confidence_scores)
            high_conf_count = len([c for c in confidence_scores if c >= 0.7])
            
            print(f"\nğŸ“Š ì‹ ë¢°ë„ ë¶„ì„")
            print(f"í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
            print(f"ê³ ì‹ ë¢°ë„ (â‰¥0.7): {high_conf_count}ê°œ ({high_conf_count/len(results)*100:.1f}%)")
        
        # ì²˜ë¦¬ ì‹œê°„ ë¶„ì„
        if processing_times:
            avg_time = np.mean(processing_times)
            min_time = np.min(processing_times)
            max_time = np.max(processing_times)
            
            print(f"\nâ±ï¸ ì²˜ë¦¬ ì‹œê°„ ë¶„ì„")
            print(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ì´ˆ")
            print(f"ìµœì†Œ/ìµœëŒ€: {min_time:.2f}ì´ˆ / {max_time:.2f}ì´ˆ")
            print(f"ì˜ˆìƒ ì „ì²´ ì‹œê°„: {(avg_time * 515) / 60:.1f}ë¶„")
        
        # ë‹µë³€ ë¶„í¬ ë¶„ì„ (ê°ê´€ì‹)
        if mc_results:
            print(f"\nğŸ“ˆ ê°ê´€ì‹ ë‹µë³€ ë¶„í¬")
            total_mc = len(mc_results)
            for choice in sorted(answer_distribution.keys()):
                count = answer_distribution[choice]
                pct = (count / total_mc * 100) if total_mc > 0 else 0
                print(f"  {choice}ë²ˆ: {count}ê°œ ({pct:.1f}%)")
            
            # í¸í–¥ ê²€ì‚¬
            max_choice = max(answer_distribution, key=answer_distribution.get)
            max_pct = (answer_distribution[max_choice] / total_mc) * 100
            if max_pct > 50:
                print(f"âš ï¸ ë‹µë³€ í¸í–¥ ê°ì§€: {max_choice}ë²ˆ {max_pct:.1f}%")
            else:
                print("âœ… ë‹µë³€ ë¶„í¬ ê· í˜•ì ")
        
        # ë‚œì´ë„ë³„ ë¶„ì„
        difficulty_stats = {}
        for result in results:
            diff = result["difficulty"]
            if diff < 0.3:
                category = "ì‰¬ì›€"
            elif diff < 0.7:
                category = "ë³´í†µ"
            else:
                category = "ì–´ë ¤ì›€"
            
            if category not in difficulty_stats:
                difficulty_stats[category] = {"count": 0, "avg_conf": []}
            difficulty_stats[category]["count"] += 1
            difficulty_stats[category]["avg_conf"].append(result["confidence"])
        
        print(f"\nğŸ“Š ë‚œì´ë„ë³„ ë¶„ì„")
        for category, stats in difficulty_stats.items():
            avg_conf = np.mean(stats["avg_conf"]) if stats["avg_conf"] else 0
            print(f"  {category}: {stats['count']}ê°œ, í‰ê· ì‹ ë¢°ë„ {avg_conf:.3f}")
    
    def _analyze_speed_results(self, results: list, total_time: float, sample_size: int):
        """ì†ë„ ê²°ê³¼ ë¶„ì„"""
        
        print(f"\nâš¡ ì†ë„ ë¶„ì„ ê²°ê³¼")
        print(f"{'='*50}")
        
        print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"í‰ê·  ì²˜ë¦¬ ì†ë„: {total_time/sample_size:.2f}ì´ˆ/ë¬¸í•­")
        print(f"ì²˜ë¦¬ ì†ë„: {sample_size/(total_time/60):.1f}ë¬¸í•­/ë¶„")
        
        # ì „ì²´ ì˜ˆìƒ ì‹œê°„
        estimated_total_time = (total_time / sample_size) * 515
        print(f"ì˜ˆìƒ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {estimated_total_time/60:.1f}ë¶„")
        
        # ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€
        questions_per_minute = sample_size / (total_time / 60)
        if questions_per_minute > 30:
            performance_grade = "Sê¸‰ (ì´ˆê³ ì†)"
        elif questions_per_minute > 20:
            performance_grade = "Aê¸‰ (ê³ ì†)"
        elif questions_per_minute > 10:
            performance_grade = "Bê¸‰ (ë³´í†µ)"
        else:
            performance_grade = "Cê¸‰ (ê°œì„ í•„ìš”)"
        
        print(f"ì„±ëŠ¥ ë“±ê¸‰: {performance_grade}")
        
        # ì‹œê°„ ì—¬ìœ  ë¶„ì„
        time_limit_minutes = 270  # 4ì‹œê°„ 30ë¶„
        safety_margin = time_limit_minutes - (estimated_total_time / 60)
        print(f"ì‹œê°„ ì—¬ìœ : {safety_margin:.1f}ë¶„")
        
        if safety_margin > 60:
            print("âœ… ì¶©ë¶„í•œ ì‹œê°„ ì—¬ìœ ")
        elif safety_margin > 30:
            print("âš ï¸ ì ë‹¹í•œ ì‹œê°„ ì—¬ìœ ")
        else:
            print("âŒ ì‹œê°„ ë¶€ì¡± ìœ„í—˜")
    
    def _analyze_stress_test_results(self, processed_count: int, error_count: int, 
                                   performance_history: list, duration_minutes: int):
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„"""
        
        print(f"\nğŸ”¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print(f"{'='*50}")
        
        print(f"í…ŒìŠ¤íŠ¸ ì‹œê°„: {duration_minutes}ë¶„")
        print(f"ì²˜ë¦¬ëœ ë¬¸í•­: {processed_count}ê°œ")
        print(f"ì˜¤ë¥˜ ë°œìƒ: {error_count}ê°œ")
        print(f"ì„±ê³µë¥ : {((processed_count-error_count)/processed_count)*100:.1f}%")
        
        if performance_history:
            # ì„±ëŠ¥ ì¶”ì´ ë¶„ì„
            processing_times = [p["processing_time"] for p in performance_history]
            confidences = [p["confidence"] for p in performance_history]
            
            print(f"\nğŸ“ˆ ì„±ëŠ¥ ì¶”ì´")
            print(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {np.mean(processing_times):.2f}ì´ˆ")
            print(f"ì²˜ë¦¬ ì‹œê°„ í‘œì¤€í¸ì°¨: {np.std(processing_times):.2f}ì´ˆ")
            print(f"í‰ê·  ì‹ ë¢°ë„: {np.mean(confidences):.3f}")
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (CUDA ì‚¬ìš© ì‹œ)
            if torch.cuda.is_available() and performance_history:
                gpu_memories = [p["gpu_memory"] for p in performance_history]
                print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {np.mean(gpu_memories):.1f}GB (í‰ê· )")
        
        # ì•ˆì •ì„± í‰ê°€
        error_rate = error_count / processed_count if processed_count > 0 else 1
        if error_rate < 0.01:
            stability_grade = "ë§¤ìš° ì•ˆì •ì "
        elif error_rate < 0.05:
            stability_grade = "ì•ˆì •ì "
        elif error_rate < 0.1:
            stability_grade = "ë³´í†µ"
        else:
            stability_grade = "ë¶ˆì•ˆì •"
        
        print(f"ì‹œìŠ¤í…œ ì•ˆì •ì„±: {stability_grade}")
    
    def run_comprehensive_benchmark(self):
        """ì¢…í•© ë²¤ì¹˜ë§ˆí¬"""
        
        print(f"\nğŸ† ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        print(f"{'='*60}")
        
        benchmark_results = {}
        
        # 1. ì •í™•ë„ í…ŒìŠ¤íŠ¸
        print("\n1ï¸âƒ£ ì •í™•ë„ í…ŒìŠ¤íŠ¸ (20ê°œ ìƒ˜í”Œ)")
        accuracy_results = self.run_comprehensive_accuracy_test(20)
        benchmark_results["accuracy"] = {
            "sample_count": len(accuracy_results),
            "avg_confidence": np.mean([r["confidence"] for r in accuracy_results]),
            "high_confidence_rate": len([r for r in accuracy_results if r["confidence"] >= 0.7]) / len(accuracy_results)
        }
        
        # 2. ì†ë„ í…ŒìŠ¤íŠ¸
        print("\n2ï¸âƒ£ ì†ë„ í…ŒìŠ¤íŠ¸ (50ê°œ ìƒ˜í”Œ)")
        speed_results = self.run_ultra_speed_test(50)
        benchmark_results["speed"] = {
            "sample_count": len(speed_results),
            "avg_time": np.mean([r["time"] for r in speed_results]),
            "questions_per_minute": len(speed_results) / (sum([r["time"] for r in speed_results]) / 60)
        }
        
        # 3. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸
        print("\n3ï¸âƒ£ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸")
        memory_results = self._test_memory_efficiency()
        benchmark_results["memory"] = memory_results
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        total_score = self._calculate_benchmark_score(benchmark_results)
        
        # ìµœì¢… ë³´ê³ ì„œ
        self._generate_benchmark_report(benchmark_results, total_score)
        
        return benchmark_results
    
    def _test_memory_efficiency(self) -> dict:
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸"""
        
        if not torch.cuda.is_available():
            return {"status": "CUDA ì—†ìŒ"}
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        torch.cuda.empty_cache()
        initial_memory = torch.cuda.memory_allocated() / (1024**3)
        
        # ì‘ì—… ë¶€í•˜ ìƒì„±
        test_df = pd.read_csv('./test.csv')
        sample_questions = test_df['Question'].head(10).tolist()
        
        max_memory = initial_memory
        
        for question in sample_questions:
            structure = self.data_processor.analyze_question_structure(question)
            prompt = self.prompt_engineer.create_adaptive_prompt(
                question, structure["question_type"], {}, "simple"
            )
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
            current_memory = torch.cuda.memory_allocated() / (1024**3)
            max_memory = max(max_memory, current_memory)
        
        torch.cuda.empty_cache()
        final_memory = torch.cuda.memory_allocated() / (1024**3)
        
        return {
            "initial_memory_gb": round(initial_memory, 2),
            "max_memory_gb": round(max_memory, 2),
            "final_memory_gb": round(final_memory, 2),
            "memory_efficiency": round((final_memory - initial_memory) / max_memory, 3),
            "total_gpu_memory_gb": torch.cuda.get_device_properties(0).total_memory / (1024**3)
        }
    
    def _calculate_benchmark_score(self, results: dict) -> float:
        """ë²¤ì¹˜ë§ˆí¬ ì ìˆ˜ ê³„ì‚°"""
        
        score = 0
        
        # ì •í™•ë„ ì ìˆ˜ (40ì )
        if "accuracy" in results:
            acc = results["accuracy"]
            accuracy_score = (acc["avg_confidence"] * 30) + (acc["high_confidence_rate"] * 10)
            score += min(accuracy_score, 40)
        
        # ì†ë„ ì ìˆ˜ (35ì )
        if "speed" in results:
            speed = results["speed"]
            questions_per_min = speed["questions_per_minute"]
            if questions_per_min > 30:
                speed_score = 35
            elif questions_per_min > 20:
                speed_score = 30
            elif questions_per_min > 10:
                speed_score = 20
            else:
                speed_score = 10
            score += speed_score
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì ìˆ˜ (15ì )
        if "memory" in results and "memory_efficiency" in results["memory"]:
            memory_eff = results["memory"]["memory_efficiency"]
            if memory_eff < 0.3:
                memory_score = 15
            elif memory_eff < 0.5:
                memory_score = 12
            elif memory_eff < 0.7:
                memory_score = 8
            else:
                memory_score = 5
            score += memory_score
        
        # ì‹œìŠ¤í…œ ì•ˆì •ì„± ì ìˆ˜ (10ì )
        # GPU ë©”ëª¨ë¦¬ í¬ê¸°ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            if gpu_memory >= 20:
                score += 10  # ëŒ€ìš©ëŸ‰ GPU ë³´ë„ˆìŠ¤
            elif gpu_memory >= 12:
                score += 8
            else:
                score += 5
        
        return min(score, 100)
    
    def _generate_benchmark_report(self, results: dict, total_score: float):
        """ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ ìƒì„±"""
        
        print(f"\nğŸ† ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ")
        print(f"{'='*60}")
        
        print(f"ì´ì : {total_score:.1f}/100")
        
        # ë“±ê¸‰ íŒì •
        if total_score >= 90:
            grade = "Sê¸‰ (ìµœìš°ìˆ˜)"
            comment = "ğŸ¥‡ ìµœì ì˜ ì„±ëŠ¥! ëŒ€íšŒ ì¤€ë¹„ ì™„ë£Œ"
        elif total_score >= 80:
            grade = "Aê¸‰ (ìš°ìˆ˜)"
            comment = "ğŸ¥ˆ ìš°ìˆ˜í•œ ì„±ëŠ¥! ì•½ê°„ì˜ ì¡°ì •ìœ¼ë¡œ ì™„ë²½"
        elif total_score >= 70:
            grade = "Bê¸‰ (ì–‘í˜¸)"
            comment = "ğŸ¥‰ ì–‘í˜¸í•œ ì„±ëŠ¥! ì¼ë¶€ ê°œì„  ê¶Œì¥"
        elif total_score >= 60:
            grade = "Cê¸‰ (ë³´í†µ)"
            comment = "âš ï¸ ë³´í†µ ì„±ëŠ¥, ìµœì í™” í•„ìš”"
        else:
            grade = "Dê¸‰ (ê°œì„ í•„ìš”)"
            comment = "âŒ ì„±ëŠ¥ ê°œì„  í•„ìš”"
        
        print(f"ì„±ëŠ¥ ë“±ê¸‰: {grade}")
        print(f"í‰ê°€: {comment}")
        
        # ì„¸ë¶€ ì ìˆ˜
        print(f"\nğŸ“Š ì„¸ë¶€ ì ìˆ˜")
        if "accuracy" in results:
            print(f"ì •í™•ë„: {results['accuracy']['avg_confidence']:.3f} (ì‹ ë¢°ë„)")
        if "speed" in results:
            print(f"ì†ë„: {results['speed']['questions_per_minute']:.1f} ë¬¸í•­/ë¶„")
        if "memory" in results and "memory_efficiency" in results["memory"]:
            print(f"ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {results['memory']['memory_efficiency']:.3f}")
        
        # ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­")
        recommendations = self._generate_improvement_recommendations(results, total_score)
        for rec in recommendations:
            print(f"  â€¢ {rec}")
    
    def _generate_improvement_recommendations(self, results: dict, score: float) -> list:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ì ìˆ˜ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if score < 70:
            recommendations.append("ì‹œìŠ¤í…œ ì‚¬ì–‘ ì—…ê·¸ë ˆì´ë“œ ê³ ë ¤ (GPU ë©”ëª¨ë¦¬, CPU ì„±ëŠ¥)")
        
        # ì •í™•ë„ ê¸°ë°˜
        if "accuracy" in results:
            avg_conf = results["accuracy"]["avg_confidence"]
            if avg_conf < 0.6:
                recommendations.append("í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°œì„ ìœ¼ë¡œ ì‹ ë¢°ë„ í–¥ìƒ")
                recommendations.append("ë„ë©”ì¸ íŠ¹í™” ì§€ì‹ ë² ì´ìŠ¤ í™•ì¥")
        
        # ì†ë„ ê¸°ë°˜
        if "speed" in results:
            qpm = results["speed"]["questions_per_minute"]
            if qpm < 15:
                recommendations.append("ë°°ì¹˜ í¬ê¸° ì¦ê°€ ë° ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”")
                recommendations.append("ëª¨ë¸ ì»´íŒŒì¼ ë° Mixed Precision í™œìš©")
        
        # ë©”ëª¨ë¦¬ ê¸°ë°˜
        if "memory" in results and "memory_efficiency" in results["memory"]:
            if results["memory"]["memory_efficiency"] > 0.5:
                recommendations.append("ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°œì„  (ìºì‹œ ì •ë¦¬, ë°°ì¹˜ í¬ê¸° ì¡°ì •)")
        
        # ì‹œìŠ¤í…œë³„ ê¶Œì¥ì‚¬í•­
        performance_tier = self.system_info["performance_tier"]
        if performance_tier == "Medium":
            recommendations.append("GPU ë©”ëª¨ë¦¬ ë¶€ì¡± - ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ í™œì„±í™”")
        elif performance_tier == "Basic":
            recommendations.append("í•˜ë“œì›¨ì–´ ì—…ê·¸ë ˆì´ë“œ ê°•ë ¥ ê¶Œì¥")
        
        if not recommendations:
            recommendations.append("í˜„ì¬ ì„¤ì •ì´ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
        
        return recommendations
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        print(f"\nğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        # ì»´í¬ë„ŒíŠ¸ ì •ë¦¬
        if hasattr(self, 'model_handler'):
            self.model_handler.cleanup()
        if hasattr(self, 'data_processor'):
            self.data_processor.cleanup()
        if hasattr(self, 'prompt_engineer'):
            self.prompt_engineer.cleanup()
        if hasattr(self, 'knowledge_base'):
            self.knowledge_base.cleanup()
        if hasattr(self, 'pattern_learner'):
            self.pattern_learner.cleanup()
        if hasattr(self, 'answer_selector'):
            self.answer_selector.cleanup()
        
        # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
        total_time = time.time() - self.start_time
        print(f"ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.1f}ì´ˆ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    parser = argparse.ArgumentParser(description='ê°œë°œ ë„êµ¬')
    parser.add_argument('--test-type', type=str, default='accuracy',
                       choices=['accuracy', 'speed', 'stress', 'benchmark', 'all'],
                       help='í…ŒìŠ¤íŠ¸ ìœ í˜•')
    parser.add_argument('--sample-size', type=int, default=30,
                       help='í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜')
    parser.add_argument('--duration', type=int, default=5,
                       help='ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œê°„ (ë¶„)')
    
    args = parser.parse_args()
    
    # ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
    if not torch.cuda.is_available():
        print("âŒ CUDA ì—†ìŒ - GPU ì¶”ë¡  ë¶ˆê°€ëŠ¥")
        return
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    print(f"ğŸš€ GPU: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")
    
    # ë°ì´í„° íŒŒì¼ í™•ì¸
    if not os.path.exists('./test.csv') or not os.path.exists('./sample_submission.csv'):
        print("âŒ ë°ì´í„° íŒŒì¼ ì—†ìŒ")
        return
    
    # ëª¨ë¸ ì„¤ì • (ë™ì  ìµœì í™”)
    if gpu_memory >= 20:
        model_config = {
            "model_name": "upstage/SOLAR-10.7B-Instruct-v1.0",
            "device": "cuda",
            "load_in_4bit": False,
            "max_memory_gb": int(gpu_memory * 0.9)
        }
        print("ğŸ¯ ì´ˆê³ ì„±ëŠ¥ ëª¨ë“œ ì„¤ì •")
    elif gpu_memory >= 12:
        model_config = {
            "model_name": "upstage/SOLAR-10.7B-Instruct-v1.0",
            "device": "cuda", 
            "load_in_4bit": False,
            "max_memory_gb": int(gpu_memory * 0.85)
        }
        print("ğŸ¯ ê³ ì„±ëŠ¥ ëª¨ë“œ ì„¤ì •")
    else:
        model_config = {
            "model_name": "upstage/SOLAR-10.7B-Instruct-v1.0",
            "device": "cuda",
            "load_in_4bit": True,  # ë©”ëª¨ë¦¬ ì ˆì•½
            "max_memory_gb": int(gpu_memory * 0.8)
        }
        print("ğŸ¯ íš¨ìœ¨ì„± ëª¨ë“œ ì„¤ì •")
    
    # í…ŒìŠ¤í„° ì´ˆê¸°í™” ë° ì‹¤í–‰
    tester = None
    try:
        tester = UltraHighPerformanceTester(model_config)
        
        if args.test_type == 'accuracy':
            tester.run_comprehensive_accuracy_test(args.sample_size)
        elif args.test_type == 'speed':
            tester.run_ultra_speed_test(args.sample_size)
        elif args.test_type == 'stress':
            tester.run_stress_test(args.duration)
        elif args.test_type == 'benchmark':
            tester.run_comprehensive_benchmark()
        elif args.test_type == 'all':
            print("ğŸ”¥ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
            tester.run_comprehensive_accuracy_test(20)
            tester.run_ultra_speed_test(30)
            tester.run_stress_test(3)
        
        print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if tester:
            tester.cleanup()

if __name__ == "__main__":
    main()